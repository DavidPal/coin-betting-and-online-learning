% \documentclass[anon]{colt2016} % Anonymized submission
\documentclass{colt2016} % NON-anonymized submission

\usepackage{amsmath,amssymb}
\usepackage[nolist]{acronym}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{times}
\usepackage{enumerate}

%\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Wealth}{Wealth}
\DeclareMathOperator{\Reward}{Reward}

\newcommand{\N}{\mathbb{N}}     % natural numbers
\newcommand{\R}{\mathbb{R}}     % real numbers
\newcommand{\C}{\mathbb{C}}     % complex numbers
\renewcommand{\H}{\mathcal{H}}  % Hilbert space
\newcommand{\KL}[2]{D({#1}\|{#2})}  % KL divergence
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{acronym}
\acro{EG}{Exponentiated Gradient}
\acro{DFEG}{Dimension-Free Exponentiated Gradient}
\acro{OMD}{Online Mirror Descent}
\acro{ASGD}{Averaged Stochastic Gradient Descent}
\acro{SGD}{Stochastic Gradient Descent}
\acro{PiSTOL}{Parameter-free STOchastic Learning}
\acro{OCO}{Online Convex Optimization}
\acro{OLO}{Online Linear Optimization}
\acro{RKHS}{Reproducing Kernel Hilbert Space}
\acro{IID}{Independent and Identically Distributed}
\acro{SVM}{Support Vector Machine}
\acro{ERM}{Empirical Risk Minimization}
\acro{COCOB}{Continous Coin Betting}
\acro{MBA}{Master Betting Algorithm}
\acro{KT}{Krichevsky-Trofimov}
\acro{OLEA}{Learning with Expert Advice}
\end{acronym}

\coltauthor{%
   \Name{Francesco Orabona} \Email{francesco@orabona.com}\\
   \Name{D\'avid P\'al} \Email{dpal@yahoo-inc.com}\\
{\addr Yahoo Labs, New York}
}

\title{From Coin Betting to Parameter-Free Online Learning}

\begin{document}

\maketitle

\begin{abstract}
In the recent years a number of parameter-free algorithms for online linear
optimization over Hilbert spaces and for learning with expert advice have been
developed. While these two families of algorithms might seem different to a
distract eye, the proof methods are indeed very similar, making the reader
wonder if such a connection is only accidental.

In this paper, we unify these two families, showing that both can be
instantiated from online coin betting algorithms. We instantiate our framework
using a betting algorithm based on the Krichevsky-Trofimov estimator. We obtain
a simple algorithm for online linear optimization with $\widetilde
O(\|u\|\sqrt{T \log\|u\|})$ regret with respect to any competitor $u$. For
learning with expert advice for any prior $\pi$ we obtain an algorithm that has
$O(\sqrt{T (1 + \KL{u}{\pi})})$ regret against any competitor $u$. In both
cases, no parameters need to be tuned.
\end{abstract}

\input{introduction}
\input{preliminaries}
\input{coin-betting-potentials}
\input{reduction-hilbert-space}
\input{reduction-experts}
\input{kt-estimator}

% Acknowledgments---Will not appear in anonymized version
\acks{The authors thank Jacob Abernethy, Nicol\`{o} Cesa-Bianchi, Satyen Kale,
and Chansoo Lee for useful discussions on this work.}

\bibliography{learning}

\appendix
\input{appendix}
\input{lambert}


\end{document}
