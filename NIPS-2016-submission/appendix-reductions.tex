\section{Proof of Lemma~\ref{lemma:recursion_hilbert}}
\label{section:hilbert-space-reduction}

First we state the following Lemma from~\cite{McMahanO14} and reported here with
our notation for completeness.

\begin{lemma}[Extremes]
\label{lemma:extremes}
Let $h:(-a,a) \to \R$ be a twice-differentiable function that
satisfies $x \cdot h''(x) \ge h'(x)$ for all $x \in [0,a)$. Let $c:[0,\infty) \to \R$
be an arbitrary function. Then, if vectors $u,v \in \H$ satisfy $\|u\| + \|v\| < a$, then
\begin{multline}
\label{equation:lemma-extremes-1}
c(\norm{u}, \norm{v}) \cdot \langle u, v \rangle - h(\norm{u+v})
\ge \min \left\{ c(\norm{u}, \norm{v}) \cdot \norm{u} \cdot \norm{v} - h(\norm{v} + \norm{v}), \right. \\
\left. - c(\norm{u}, \norm{v}) \cdot \norm{u} \cdot \norm{v} - h(\norm{u} - \norm{v}) \right\} \; .
\end{multline}
\end{lemma}
%
\begin{proof}
If $u$ or $v$ is zero, the inequality \eqref{equation:lemma-extremes-1} clearly
holds. From now on we assume that $u,v$ are non-zero. Let $\alpha$ be the cosine
of the angle of between $u$ and $v$. More formally,
$$
\alpha = \frac{\langle u, v \rangle}{\norm{u} \cdot \norm{v}} \; .
$$
With this notation, the left-hand side of \eqref{equation:lemma-extremes-1} is
$$
f(\alpha) = c(\norm{u}, \norm{v}) \cdot \alpha \norm{u} \cdot \norm{v} - h(\sqrt{\norm{u}^2 + \norm{v}^2 + 2 \alpha \norm{u} \cdot \norm{v}}) \; .
$$
Since $h$ is even, the inequality \eqref{equation:lemma-extremes-1} is equivalent to
$$
\forall \alpha \in [-1,1] \qquad \qquad f(\alpha) \ge \min \left\{f(+1), f(-1)\right\} \; .
$$
The last inequality is clearly true if $f:[-1,1] \to \R$ is concave. We now
check that $f$ is indeed concave, which we prove by showing that the second
derivative is non-positive. The first derivative of $f$ is
$$
f'(\alpha) = c(\norm{u}, \norm{v}) \cdot \|u\| \cdot \|v\| - \frac{h'(\sqrt{\|u\|^2 + \|v\|^2 + 2 \alpha \|u\| \cdot \|v\|}) \cdot \|u\| \cdot \|v\|}{\sqrt{\|u\|^2 + \|v\|^2 + 2 \alpha \|u\| \cdot \|v\|}} \; .
$$
The second derivative of $f$ is
\begin{multline*}
f''(\alpha) = - \frac{\|u\|^2 \cdot \|v\|^2}{\|u\|^2 + \|v\|^2 + 2 \alpha \|u\| \cdot \|v\|} \\
 \cdot \left( h''(\sqrt{\|u\|^2 + \|v\|^2 + 2 \alpha \|u\| \cdot \|v\|})  - \frac{h'(\sqrt{\|u\|^2 + \|v\|^2 + 2 \alpha \|u\| \cdot \|v\|})}{\sqrt{\|u\|^2 + \|v\|^2 + 2\alpha \|u\| \cdot \|v\|}}  \right) \; .
\end{multline*}
If we consider $x = \sqrt{\|u\|^2 + \|v\|^2 + 2 \alpha \|u\| \cdot \|v\|}$, the
assumption $x \cdot h''(x) \ge h'(x)$ implies that $f''(\alpha)$ is non-positive.
This finishes the proof of the inequality \eqref{equation:lemma-extremes-1}.
\end{proof}

We also need the following technical Lemma whose proof relies mainly on
property (4) of Definition~\ref{definition:potential}.
\begin{lemma}
\label{lemma:recursion_hilbert}
Let $\{F_t\}_{t=0}^\infty$ be a sequence of excellent coin-betting potentials.
Let $g_1, g_2, \dots, g_t$ be vectors in a Hilbert space $\H$ such that
$\norm{g_1}, \norm{g_2}, \dots, \norm{g_t} \le 1$. Let $\beta_t$
be defined by \eqref{equation:potential-based-strategy-hilbert-space}
and let $x = \sum_{i=1}^{t-1} g_i$. Then,
$$
\left(1 + \beta_t \frac{\langle g_t, x \rangle}{\norm{x}} \right) F_{t-1}(\norm{x})
\ge F_t(\norm{x + g_t}) \; .
$$
\end{lemma}
%
\begin{proof}
Since $F_t(x)$ is an excellent coin-betting potential, it satisfies $x
F_t''(x) \ge F_t'(x)$. Hence,
\begin{align*}
&\left(1 + \beta_t \frac{\langle g_t, x \rangle}{\norm{x}} \right) F_{t-1}(\norm{x}) - F_t(\norm{x + g_t}) \\
&\quad = F_{t-1}(\norm{x}) + \beta_t \frac{\langle g_t, x \rangle}{\norm{x}} F_{t-1}(\norm{x}) - F_t(\norm{x + g_t}) \\
&\quad \ge F_{t-1}(\norm{x})+\min_{r \in \{-1,1\}} \beta_t r \norm{g_t} F_{t-1}(\norm{x}) - F_t(\norm{x} + r \norm{g_t}) \\
&\quad =\min_{r \in \{-1,1\}} \left(1 + \beta_t r \norm{g_t}\right) F_{t-1}(\norm{x}) - F_t(\norm{x} + r \norm{g_t}) \\
&\quad \ge 0 \; .
\end{align*}
If $x \neq 0$, the first inequality comes from Lemma~\ref{lemma:extremes} with
$c(z,\cdot) = \frac{F_{t-1}(z+1) - F_{t-1}(z-1)}{F_{t-1}(z+1) + F_{t-1}(z-1)} F_{t-1}(z) / z$ and
$h(z) = F_t(z)$, $u=g_t$, $v=x$.
If $x=0$ then, according to
\eqref{equation:potential-based-strategy-hilbert-space}, $\beta_t = 0$ and the
first inequality trivially holds. The second inequality follows from the
property 3 of a coin-betting potential.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{theorem:hilbert-space-olo-regret-bound}]
Compared to the one dimensional case, the only hard part is to show an analogue of \eqref{equation:wealth-lower-bound-generic},
\begin{equation}
\label{equation:wealth-lower-bound-hilbert-space}
\Wealth_t \ge F_t\left(\norm{\sum_{t=1}^T g_t} \right) \; .
\end{equation}
To prove \eqref{equation:wealth-lower-bound-hilbert-space}, we imitate the induction
proof from Section~\ref{section:coin-betting-potentials}. The base case
$t=0$ is trivial, since both sides of the inequality are equal to $\epsilon$.
For $t \ge 1$, if we let $x = \sum_{i=1}^{t-1} g_i$, we have
\begin{align*}
\Wealth_t
&= \langle g_t, w_t \rangle + \Wealth_{t-1}
= \left(1 + \beta_t \frac{\langle g_t, x \rangle}{\norm{x}} \right) \Wealth_{t-1} \\
&\ge \left(1 + \beta_t \frac{\langle g_t, x \rangle}{\norm{x}} \right) F_{t-1}(\norm{x})
\stackrel{\text{\textbf{(?)}}}{\ge} F_t(\norm{x + g_t})
= F_t\left(\norm{\sum_{i=1}^t g_i} \right) \; .
\end{align*}
The only non-trivial inequality is marked with a question mark. The inequality
is the content of the Lemma~\ref{lemma:recursion_hilbert}. 

This establishes \eqref{equation:wealth-lower-bound-hilbert-space},
from which we immediately have a reward lower bound
\begin{equation}
\label{equation:hilbert-space-olo-reward-lower-bound}
\Reward_T
= \sum_{t=1}^T \langle g_t, w_t \rangle
= \Wealth_T \ - \ \epsilon
\ge F_T\left(\norm{\sum_{t=1}^T g_t} \right) \ - \ \epsilon \; .
\end{equation}
We apply Lemma~\ref{lemma:reward-regret} to the function $F(x) = F_T(\norm{x}) -
\epsilon$ and we are almost done. The only remaining property we need is that if
$F$ is an even function then Fenchel conjugate of $F(\norm{\cdot})$ is
$F^*(\norm{\cdot})$; see \citet[Example 13.7]{BauschkeC2011}.
\end{proof}
