\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}

\usepackage[final]{nips_2016}
% For anonymous submission use:  \usepackage{nips_2016} 
% For camera-ready version use:  \usepackage[final]{nips_2016} 

\usepackage{amssymb,amsmath,amsthm}
\usepackage[nolist]{acronym}
\usepackage{algorithm,algorithmic}
\usepackage{enumerate}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\newtheorem{definition}{Definition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{proposition}[definition]{Proposition}

\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Wealth}{Wealth}
\DeclareMathOperator{\Reward}{Reward}

\newcommand{\N}{\mathbb{N}}     % natural numbers
\newcommand{\R}{\mathbb{R}}     % real numbers
\newcommand{\C}{\mathbb{C}}     % complex numbers
\renewcommand{\H}{\mathcal{H}}  % Hilbert space
\newcommand{\KL}[2]{\D\left({#1}\middle\|{#2}\right)}  % KL divergence
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\DeclareMathOperator*{\D}{D}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\indicator}{\mathbf{1}}
\newcommand{\scO}{\mathcal{O}}

\begin{acronym}
\acro{EG}{Exponentiated Gradient}
\acro{DFEG}{Dimension-Free Exponentiated Gradient}
\acro{OMD}{Online Mirror Descent}
\acro{ASGD}{Averaged Stochastic Gradient Descent}
\acro{SGD}{Stochastic Gradient Descent}
\acro{PiSTOL}{Parameter-free STOchastic Learning}
\acro{OCO}{Online Convex Optimization}
\acro{OLO}{Online Linear Optimization}
\acro{RKHS}{Reproducing Kernel Hilbert Space}
\acro{IID}{Independent and Identically Distributed}
\acro{SVM}{Support Vector Machine}
\acro{ERM}{Empirical Risk Minimization}
\acro{COCOB}{Continous Coin Betting}
\acro{MBA}{Master Betting Algorithm}
\acro{KT}{Krichevsky-Trofimov}
\acro{LEA}{Learning with Expert Advice}
\end{acronym}

\author{
  Francesco Orabona\\
  Yahoo Research, New York\\
  \texttt{francesco@orabona.com}
  \And
  D\'avid P\'al\\
  Yahoo Research, New York\\
  \texttt{dpal@yahoo-inc.com}
}

\title{Coin Betting and Parameter-Free Online Learning}

\begin{document}

\maketitle

\begin{abstract}
In the recent years a number of parameter-free algorithms for online linear
optimization over Hilbert spaces and for learning with expert advice have been
developed. While these two families of algorithms might seem different to a
distract eye, the proof methods are indeed very similar, making the reader
wonder if such a connection is only accidental.

In this paper, we unify these two families, showing that both can be
instantiated from online coin betting algorithms. We present two new reductions
from online coin betting to online linear optimization over Hilbert spaces and
to learning with expert advice. We instantiate our framework using a betting
algorithm based on the Krichevsky-Trofimov estimator. We obtain optimal regret
bounds with simple algorithms, and no parameters to be tuned.
\end{abstract}

\input{introduction}
\input{preliminaries}
\input{coin-betting-potentials}
\input{reduction-hilbert-space}
\input{reduction-experts}
\input{kt-estimator}
\input{discussion}

% Acknowledgments: Comment out in anonymous submission.

\subsubsection*{Acknowledgments}
The authors thank Jacob Abernethy, Nicol\`{o} Cesa-Bianchi, Satyen Kale,
Chansoo Lee, and  Giuseppe Molteni for useful discussions on this work.

\bibliographystyle{plainnat}
\bibliography{learning}

\appendix
\input{appendix-coin-betting}
\input{appendix-reductions}
\input{appendix-expert-reduction}
\input{appendix-kt-potential}
\input{appendix-kt-reductions}

\end{document}
