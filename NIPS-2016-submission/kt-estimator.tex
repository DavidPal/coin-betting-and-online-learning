\section{Applications of Krichevsky-Trofimov Estimator}
\label{section:kt-estimator}

In the previous two sections we have shown that a coin-betting potential with a
guaranteed rapid growth of the wealth will give good regret guarantees for
\ac{OLO} and \ac{LEA}. In this section, we show that the optimal
Krichevsky-Trofimov (KT) estimator has associated a sequence of excellent
coin-betting potentials, which we call \emph{KT potentials}. We then prove
corollaries of the regret bounds for \ac{OLO} over Hilbert space and \ac{LEA}
from Sections~\ref{section:reduction_hilbert} and~\ref{section:reduction-experts}, obtaining optimal regret bounds.

The potential corresponding to adaptive Kelly betting strategy
$\beta_t$ defined by \eqref{equation:kt-estimator-betting-strategy}
based on the KT estimator is
\begin{equation}
\label{equation:kt-estimator-potential}
F_t(x) = \epsilon \tfrac{2^t \cdot \Gamma \left( \tfrac{t+1}{2} + \frac{x}{2} \right) \cdot \Gamma \left( \tfrac{t+1}{2} - \frac{x}{2} \right)}{\pi \cdot t!}
\qquad \qquad \text{$t \ge 0$, \quad $x \in \left(-t-1, t+1\right)$,}
\end{equation}
where $\Gamma(x) = \int_0^\infty t^{-x} e^{-t} dt$ is Euler's gamma function.
The potential was introduced by~\citet{Krichevsky-Trofimov-1981} who used it
for proving regret bound for online prediction with log-loss; see also
\cite[Section 9.7]{Cesa-Bianchi-Lugosi-2006}.

This potential has also the peculiar property to satisfy the inequality in (c) of Definition~\ref{definition:potential}
with equality when $g_t\in \{-1,1\}$, i.e. $F_t(x+g_t)=(1+g_t \beta_t) \, F_{t-1}(x)$.
In fact, Theorem~\ref{theorem:kt-potential} in
Appendix~\ref{section:properties-kt-potential} shows that
\eqref{equation:kt-estimator-potential} is a sequence of excellent coin-betting
potentials for initial endowment $\epsilon$. Theorem~\ref{theorem:kt-potential}
also shows that the KT betting strategy $\beta_t$ as defined by
\eqref{equation:kt-estimator-betting-strategy} satisfies \eqref{equation:potential-based-strategy}.


\subsection{OLO in Hilbert Space}
\label{section:kt-olo}

\begin{algorithm}[t]
\caption{Algorithm for OLO over Hilbert space $\H$ based on KT potential
\label{algorithm:kt-hilbert-space-olo}}
\begin{algorithmic}[1]
{
\REQUIRE{Initial endowment $\epsilon > 0$}
\FOR{$t=1,2,\dots$}
\STATE{Predict with $w_t \leftarrow \tfrac{1}{t} \left(\epsilon + \sum_{i=1}^{t-1} \langle g_i, w_i \rangle \right) \sum_{i=1}^{t-1} g_i$}
\STATE{Receive reward vector $g_t \in \H$ such that $\norm{g_t} \le 1$}
\ENDFOR
}
\end{algorithmic}
\end{algorithm}

We apply the KT potential for the construction of an OLO algorithm over a Hilbert
space $\H$. According to \eqref{equation:hilbert-space-olo}, the resulting algorithm predicts
in round $t$,
\[
w_t = \beta_t \Wealth_{t-1} \tfrac{\sum_{i=1}^{t-1} g_i}{\norm{\sum_{i=1}^{t-1} g_i}}
\]
where $\beta_t$ is defined by
\eqref{equation:potential-based-strategy-hilbert-space}. According to
Theorem~\ref{theorem:kt-potential} in Appendix
\ref{section:properties-kt-potential}, the formula for $\beta_t$ simplifies to
$\beta_t = \frac{\norm{\sum_{i=1}^{t-1} g_i}}{t}$. Hence, the prediction can be
simply written as
\[
w_t
= \tfrac{1}{t} \Wealth_{t-1} \sum_{i=1}^{t-1} g_i
= \tfrac{1}{t} \left(\epsilon + \sum_{i=1}^{t-1} \langle g_i, w_i \rangle \right) \sum_{i=1}^{t-1} g_i \; .
\]
The algorithm is stated as Algorithm~\ref{algorithm:kt-hilbert-space-olo}.  We
derive a regret bound for it as a very simple corollary of
Theorem~\ref{theorem:hilbert-space-olo-regret-bound} to the KT potential
\eqref{equation:kt-estimator-potential}. The only technical
part of the proof, in Appendix~\ref{section:corollaries_reductions}, is an upper bound on Fenchel conjugate $F_t^*$ since it cannot
be expressed as an elementary function.
%
\begin{corollary}[Regret Bound for Algorithm~\ref{algorithm:kt-hilbert-space-olo}]
\label{corollary:kt-hilbert-space-olo-regret} Let $\epsilon > 0$. Let
$\{g_t\}_{t=1}^\infty$ be any sequence of reward vectors in a Hilbert space
$\H$ such that $\norm{g_t} \le 1$.
Algorithm~\ref{algorithm:kt-hilbert-space-olo} satisfies
\[
\forall \, T \ge 0 \quad
\forall u \in \H \qquad \qquad
\Regret_T(u) \le \norm{u} \sqrt{T \ln\left(1 + \tfrac{4T^2 \norm{u}^2}{\epsilon^2} \right)} + \epsilon \left(1 - \tfrac{1}{2\sqrt{T}} \right) \;.
\]
\end{corollary}
%
It is worth noting the simplicity of Algorithm~\ref{algorithm:kt-hilbert-space-olo} and the fact that the regret bound is optimal~\cite{Streeter-McMahan-2012,Orabona-2013}. Also, $\epsilon$ can be safely set to any constant, affecting the bound only in constant terms: Its role is equivalent to the initial number of rounds used in doubling tricks.


\subsection{Learning with Expert Advice}
\label{section:kt-lea}

\begin{algorithm}[t]
\begin{algorithmic}[1]
\caption{Algorithm for Learning with Expert Advice based on shifted KT potential
\label{algorithm:kt-experts}}
{
\REQUIRE{Number of experts $N$, number of rounds $T$, prior distribution $\pi \in \Delta_N$}
\FOR{$t=1,2,\dots,T$}
\STATE{For each $i \in [N]$, set $w_{t,i} \leftarrow \tfrac{\sum_{j=1}^{t-1} \widetilde g_{j,i}}{t+T/2} \left(1 + \sum_{j=1}^{t-1} \widetilde g_{j,i} w_{j,i} \right)$}
\STATE{For each $i \in [N]$, set $\widehat{p}_{t,i} \leftarrow \pi_i [w_{t,i}]_+$}
\STATE{Predict with $p_t \leftarrow
\begin{cases}
\widehat{p}_t/\norm{\widehat{p_t}}_1 & \text{if $\norm{\widehat p_t}_1 > 0$} \\
\pi & \text{if $\norm{\widehat p_t}_1 = 0$}
\end{cases}$}
\STATE{Receive reward vector $g_t \in [0,1]^N$}
\STATE{For each $i \in [N]$, set $\widetilde g_{t,i} \leftarrow \begin{cases}
g_{t,i} - \langle g_t, p_t \rangle & \text{if $w_{t,i} > 0$} \\
[g_{t,i} - \langle g_t, p_t \rangle]_+ & \text{if $w_{t,i} \le 0$}
\end{cases}$}
\ENDFOR
}
\end{algorithmic}
\end{algorithm}

We will now construct an algorithm for \ac{LEA} based on \emph{shifted KT
potential}. The shifted potential and the resulting algorithm requires to know
the number of rounds $T$ in advance. The shifted KT potential is defined as
\[
F_t(x) = \tfrac{2^t \cdot \Gamma\left(T/2 + 1 \right) \cdot \Gamma\left(\tfrac{t+T/2+1}{2} + \frac{x}{2} \right) \cdot \Gamma\left(\tfrac{t+T/2+1}{2} - \frac{x}{2} \right)}{\Gamma\left(\tfrac{T/2+1}{2} \right)^2 \cdot \Gamma \left(t+T/2+1\right)} \; .
\]
The reason for its name is that, up to a multiplicative constant, $F_t$ is
equal to the KT potential shifted in time by $T/2$, i.e., $t$ is replaced by
$T/2+t$.  According to Theorem~\ref{theorem:kt-potential} in Appendix
\ref{section:properties-kt-potential}, the shifted KT potentials also form a
sequence of coin-betting potentials for initial endowment $1$. Furthermore, the
corresponding betting fraction is
\[
\beta_t = \tfrac{\sum_{j=1}^{t-1} \widetilde g_j}{T/2+t} \; .
\]
Recall that for construction of the final algorithm, we need, as an
intermediate step, an OLO algorithm for one-dimensional Hilbert space $\R$.
This algorithm predicts for any sequence $\{\widetilde g_t\}_{t=1}^\infty$ of reward
vectors,
\[
w_t
= \beta_t \Wealth_{t-1}
= \beta_t \left(1 + \sum_{j=1}^{t-1} \widetilde g_j w_j \right)
= \frac{\sum_{i=1}^{t-1} \widetilde g_i}{T/2+t} \left(1 + \sum_{j=1}^{t-1} \widetilde g_j w_j \right) \; .
\]
Following the construction in Section~\ref{section:reduction-experts}, we
arrive at the final algorithm, Algorithm~\ref{algorithm:kt-experts}.

We can derive a regret bound for Algorithm~\ref{algorithm:kt-experts} by
applying Theorem~\ref{theorem:regret-bound-experts} to the shifted KT
potential.  \begin{corollary}[Regret Bound for
Algorithm~\ref{algorithm:kt-experts}] \label{corollary:kt-experts-regret} Let
$N \ge 2$ and $T \ge 0$ be integers. Let $\pi \in \Delta_N$ be a prior.  For
any sequence $g_1, g_2, \dots, g_T \in [0,1]^N$ of reward vectors,
Algorithm~\ref{algorithm:kt-experts} with input $N,T,\pi$ satisfies
\[
\forall u \in \Delta_N \qquad \qquad \Regret_T(u) \le \sqrt{3T (4 + \KL{u}{\pi})} \; .
\]
\end{corollary}
Hence, the Algorithm~\ref{algorithm:kt-experts} has \emph{both} the best known guarantee on worst-case regret and per-round time complexity, see Table~\ref{table:bounds}. Also, it has the advantage to be very simple.
%As observed by \citet{Chernov-Vovk-2010}, bounds in terms of the KL
%divergence are superior to the $\epsilon$-quantile bounds.

The proof of the corollary is in the
Appendix~\ref{section:corollaries_reductions}.  The technical part of the proof
is an upper bound on $f_t^{-1}(x)$, which we conveniently do by lower bounding
$F_t(x)$.

The reason for using the shifted potential comes from the analysis of
$f_t^{-1}(x)$. The unshifted algorithm would have a $O(\sqrt{T (\log T +
\KL{u}{\pi}})$ regret bound; the shifting improves the bound to $O(\sqrt{T (1 +
\KL{u}{\pi}})$.  By changing $T/2$ in Algorithm~\ref{algorithm:kt-experts} to
another constant fraction of $T$, it is possible to trade-off between the two
constants $3$ and $4$ present in the square root.

The requirement of knowing the number of rounds $T$ in advance can be lifted by
the standard doubling trick~\citep[Section 2.3.1]{Shalev-Shwartz-2011}. We
obtain an anytime algorithm at the expense of slightly bigger leading constant,
\[
\forall \, T \ge 0 \quad \forall u \in \Delta_N \qquad \qquad
\Regret_T(u) \le \tfrac{\sqrt{2}}{\sqrt{2} - 1} \sqrt{3T (4 + \KL{u}{\pi})} \; .
\]
