\section{Introduction}
\label{section:introduction}

We consider the \ac{OLO}~\cite{Cesa-BianchiL06,Shalev-Shwartz12} setting. In each round $t$, an algorithm chooses a point
$w_t$ from a convex \emph{decision set} $K$ and then receives a loss vector
$\ell_t$. Algorithm's goal is to keep its \emph{regret} small, defined as the
difference between its cumulative loss and the cumulative loss of a fixed
strategy $u \in K$, that is
\[
\Regret_T(u) = \sum_{t=1}^T \langle \ell_t, w_t \rangle - \sum_{t=1}^T \langle \ell_t, u \rangle \; .
\]

We focus on two particular sets, the $N$-dimensional probability simplex
$\Delta_N = \{ x \in \R^N ~:~ x \ge 0, \norm{x}_1 = 1\}$ and the Hilbert space
$\H$.  \ac{OLO} over $\Delta_N$ is referred to as the problem of \ac{LEA}.  We
assume bounds on the norms of the loss vectors: For \ac{OLO} over $\H$, we
assume that $\norm{\ell_t}_2 \le 1$, and for \ac{LEA} we assume that
$\ell_t \in [0,1]^N$.

\ac{OLO} is a basic building block of many machine learning problems. For
example, \ac{OCO}, the problem analogous to \ac{OLO} where $\langle \ell_t, w_t \rangle$ is
generalized to $\ell_t(w_t)$ where $\ell_t$ is an arbitrary convex function, is
solved through a reduction to an \ac{OLO}~\cite{Shalev-Shwartz12}.
\ac{LEA}~\cite{LittlestoneW94, Vovk98,Cesa-BianchiFHHSW97} provides a way of
combining classifiers and it is at the heart of
boosting~\cite{FreundS97}. Batch and stochastic convex optimization
can be solved through a reduction to \ac{OLO}~\cite{Shalev-Shwartz12}.
Statistical learning with convex losses can also seen as stochastic convex
optimization and solved through \ac{OCO}~\cite{Munro51}, that in turns can be reduced to \ac{OLO}.

However, to achieve small regret, most of the algorithms still require to set
hyperparameters (e.g., learning rates, regularization weights) in
order to achieve the best possible theoretical and empirical performance.
Recently, a new family of parameter-free algorithms that has been proposed, both for \ac{LEA}~\cite{ChaudhuriFH09,ChernovV10,LuoS14,LuoS15,KoolenE15}
and for \ac{OLO}/\ac{OCO} over Hilbert spaces~\cite{StreeterM12,Orabona13, McMahanA13,McMahanO14,Orabona14}.
These algorithms adapt to the number of experts and to the norm of the optimal
predictor, respectively, without the need to tune parameters. 
Given the connections between \ac{OLO}/\ac{LEA} and machine learning, these algorithms
allow to design parameter-free batch machine learning algorithms through
straightforward reductions~\cite{Orabona14,LuoS15}.
Yet, not all of them are practical algorithms and/or have optimal regret bounds, see Table~\ref{tab:bounds}.
Alos, surprisingly enough, these two families of algorithms are very similar, and, a part for the non-constructive framework in \cite{FosterRS15}, there is no unifying framework.

Our contributions are as follows. We claim that a more fundamental notion
subsumes both \ac{OLO} and \ac{LEA} parameter-free algorithms. This notion is
linked to the ability to repeatedly bet on an outcome of a coin
flip. This new notion allows us to design novel parameter-free algorithms. In particular, the resulting algorithms are simple and have optimal guarantees, Table~\ref{tab:bounds}. We also show some applications of our results to convex optimization and machine learning.

\begin{table}
\begin{center}
\begin{small}
\begin{tabular}{l c c c}
\toprule
Algorithm & Worst-case regret guarantee& \begin{tabular}{@{}c@{}}Closed form \\  update\end{tabular} & Adaptive \\
\midrule
\cite{AbernethyBRT08} & $U \sqrt{T}$ for $u \in \H: \norm{u} \leq U$, $\scO(T)$ otherwise & Yes & No \\
OGD, fixed $\eta$ \cite{Shalev-Shwartz12} & $\scO((1 + \norm{u}^2)\sqrt{T})$ for $u \in \H$ & Yes & No \\
This paper, Sec.~\ref{sec:kt-olo} & $\scO(\norm{u}\sqrt{T \ln(1+\norm{u}T)}+1)$ for $u \in \H$ & Yes & Yes \\
\midrule
Hegde, fixed $\eta$ & $\scO\left(\sqrt{T (\ln(N)-U)}\right)$ for $u \in \Delta_N: H(u) \geq U$, & Yes & No \\
\cite{ChaudhuriFH09}  & $\scO\left(\sqrt{T \KL{u}{\tfrac{1}{N}\boldsymbol{1}}}+\ln^2 N\right)$ for $u \in \Delta_N$ & No & Yes \\
\cite{ChernovV10} & $\scO\left(\sqrt{T}\left(\sqrt{\KL{u}{\tfrac{1}{N}\boldsymbol{1}}}+1\right)\right)$ for $u \in \Delta_N$ & No & Yes \\
\cite{LuoS15,KoolenE15} & $\scO\left(\sqrt{T \left(\ln \ln(T)+\KL{u}{\tfrac{1}{N}\boldsymbol{1}}\right)}\right)$ for $u \in \Delta_N$ & Yes & Yes \\
\cite{FosterRS15} & $\scO\left(\sqrt{T \left(1+\KL{u}{\tfrac{1}{N}\boldsymbol{1}}\right)}\right)$ for $u \in \Delta_N$ & No & Yes \\
This paper, Sec.~\ref{sec:kt-lea} & $\scO\left(\sqrt{T \left(1+\KL{u}{\tfrac{1}{N}\boldsymbol{1}}\right)}\right)$ for $u \in \Delta_N$ & Yes & Yes \\
\bottomrule
\end{tabular}
\caption{Bla.}
\label{tab:bounds}
\end{small}
\end{center}
\end{table}