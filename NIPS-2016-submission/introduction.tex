\section{Introduction}
\label{section:introduction}

We consider the \ac{OLO}~\cite{Cesa-Bianchi-Lugosi-2006, Shalev-Shwartz-2011}
setting. In each round $t$, an algorithm chooses a point $w_t$ from a convex
\emph{decision set} $K$ and then receives a reward vector $g_t$. Algorithm's
goal is to keep its \emph{regret} small, defined as the difference between its
cumulative reward and the cumulative reward of a fixed strategy $u \in K$, that
is
\[
\Regret_T(u) = \sum_{t=1}^T \langle g_t, u \rangle - \sum_{t=1}^T \langle g_t, w_t \rangle \; .
\]

We focus on two particular sets, the $N$-dimensional probability simplex
$\Delta_N = \{ x \in \R^N ~:~ x \ge 0, \norm{x}_1 = 1\}$ and the Hilbert space
$\H$.  \ac{OLO} over $\Delta_N$ is referred to as the problem of \ac{LEA}.  We
assume bounds on the norms of the reward vectors: For \ac{OLO} over $\H$, we
assume that $\norm{g_t} \le 1$, and for \ac{LEA} we assume that $g_t \in
[0,1]^N$.

\ac{OLO} is a basic building block of many machine learning problems. For
example, \ac{OCO}, the problem analogous to \ac{OLO} where $-\langle g_t, w_t
\rangle$ is generalized to $\ell_t(w_t)$ where $\ell_t$ is an arbitrary convex
function, is solved through a reduction to an
\ac{OLO}~\cite{Shalev-Shwartz-2011}.  \ac{LEA}~\cite{Littlestone-Warmuth-1994,
Vovk-1998, Cesa-Bianchi-Freund-Haussler-Helmbold-Schapire-Warmuth-1997}
provides a way of combining classifiers and it is at the heart of
boosting~\cite{Freund-Schapire-1997}. Batch and stochastic convex optimization
can be solved through a reduction to \ac{OLO}~\cite{Shalev-Shwartz-2011}.
Statistical learning with convex losses can also seen as stochastic convex
optimization and solved through \ac{OCO}~\cite{Munro-1951}, that in turns can
be reduced to \ac{OLO}.

However, to achieve small regret, most of the algorithms still require to set
hyperparameters (e.g., learning rates, regularization weights) in order to
achieve the best possible theoretical and empirical performance.  Recently, a
new family of parameter-free algorithms that has been proposed, both for
\ac{LEA}~\cite{Chaudhuri-Freund-Hsu-2009, Chernov-Vovk-2010, Luo-Schapire-2014,
Luo-Schapire-2015, Koolen-van-Erven-2015} and for \ac{OLO}/\ac{OCO} over
Hilbert spaces~\cite{Streeter-McMahan-2012, Orabona-2013,
McMahan-Abernethy-2013, McMahan-Orabona-2014, Orabona-2014}.  These algorithms
adapt to the number of experts and to the norm of the optimal predictor,
respectively, without the need to tune parameters.  Surprisingly, these two
families of algorithms are very similar as shown in the non-constructive
framework of~\cite{Foster-Rakhlin-Sridharan-2015}.  Given the connections
between \ac{OLO}/\ac{LEA} and machine learning, these algorithms allow to
design parameter-free batch machine learning algorithms through straightforward
reductions~\cite{Orabona-2014, Luo-Schapire-2015}.  Yet, all of existing
algorithms for LEA either have sub-optimal regret bound (e.g. extra $O(\log
\log T)$ factor) or sub-optimal running time (e.g. requiring solving a
numerical problem in every round, or extra $O(\log \log N)$ factor); see
Table~\ref{table:bounds}.

\textbf{Contributions.} We show that a more fundamental notion subsumes both
\ac{OLO} and \ac{LEA} parameter-free algorithms. We show that the ability to
maximize the wealth in bets on the outcomes of coin flips \emph{implies}
\ac{OLO} and \ac{LEA} parameter-free algorithms.  Hence, we present a novel
framework to characterize betting algorithms through potential functions and we
instantiate it with the Krichevsky-Trofimov estimator.  The resulting
algorithms are new, simple, and natural. They also have optimal guarantees on time and regret, see
Table~\ref{table:bounds}.

\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l c c c c}
\toprule
Algorithm & Worst-case regret guarantee& \begin{tabular}{@{}c@{}}Per-Round Time\\Complexity\end{tabular} & Adaptive & \begin{tabular}{@{}c@{}}Unified \\  design\end{tabular}\\
\midrule
\cite{Abernethy-Bartlett-Rakhlin-Tewari-2008} & \begin{tabular}{@{}c@{}}$U \sqrt{T}$ for any $u \in \H$ s.t. $\norm{u} \le U$\\
$\scO(T)$ otherwise\end{tabular} & $\scO(1)$ &  \\
OGD, $\eta=\tfrac{1}{\sqrt{T}}$ \cite{Shalev-Shwartz-2011} & $\scO((1 + \norm{u}^2)\sqrt{T})$, $\forall u \in \H$ & $\scO(1)$ &  \\
\cite{McMahan-Orabona-2014} & $\scO(\norm{u}\sqrt{T \ln(1+\norm{u}T)})$, $\forall u \in \H$ & $\scO(1)$ & \checkmark \\
This paper, Sec.~\ref{section:kt-olo} & $\scO(\norm{u}\sqrt{T \ln(1+\norm{u}T)})$, $\forall u \in \H$ & $\scO(1)$ & \checkmark & \checkmark\\
\midrule
Hegde~\cite{Freund-Schapire-1997}, $\eta=\sqrt{\tfrac{\ln N - U}{T}}$ & $\scO\left(\sqrt{T (\ln N -U)}\right)$ for any $u \in \Delta_N$ s.t. $H(u) \geq U$ & $\scO(N)$ &  \\
\cite{Chaudhuri-Freund-Hsu-2009}  & $\scO\left(\sqrt{T \KL{u}{\tfrac{1}{N}\boldsymbol{1}}}+\ln^2 N\right)$, $\forall u \in \Delta_N$ & $\scO(N\,K)$\footnotemark[1]& \checkmark \\
\cite{Chernov-Vovk-2010} & $\scO\left(\sqrt{T \left(1+\KL{u}{\tfrac{1}{N}\boldsymbol{1}}\right)}\right)$, $\forall u \in \Delta_N$ & $\scO(N\,K)$\footnotemark[1] & \checkmark \\
\cite{Chernov-Vovk-2010, Luo-Schapire-2015,Koolen-van-Erven-2015} & $\scO\left(\sqrt{T \left(\ln \ln T+\KL{u}{\tfrac{1}{N}\boldsymbol{1}}\right)}\right)$, $\forall u \in \Delta_N$ & $\scO(N)$ & \checkmark \\
\cite{Foster-Rakhlin-Sridharan-2015} & $\scO\left(\sqrt{T \left(1+\KL{u}{\tfrac{1}{N}\boldsymbol{1}}\right)}\right)$, $\forall u \in \Delta_N$ & $\scO(N \max_{u \in \Delta_N} \ln \KL{u}{\pi})$\footnotemark[2] & \checkmark & \checkmark \\
This paper, Sec.~\ref{section:kt-lea} & $\scO\left(\sqrt{T \left(1+\KL{u}{\tfrac{1}{N}\boldsymbol{1}}\right)}\right)$, $\forall u \in \Delta_N$ & $\scO(N)$ & \checkmark & \checkmark\\
\bottomrule
\end{tabular}}
\caption{Algorithms for \ac{OLO} over Hilbert space and \ac{LEA}}
\label{table:bounds}
\end{center}
\end{table}
\footnotetext[1]{These algorithms require to solve a 1-d numberical problem at each step. Hence, $K$ is the number of steps needed to reach the required precision. Yet, in these papers $K$ is not calculated.}
\footnotetext[2]{The computational inefficient algorithm in \cite{Foster-Rakhlin-Sridharan-2015} can be modified to achieve the stated complexity~\cite{Daniely:private}.}