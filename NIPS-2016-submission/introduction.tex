\section{Introduction}
\label{section:introduction}

\vspace{-0.2cm}

We consider the \ac{OLO}~\cite{Cesa-Bianchi-Lugosi-2006, Shalev-Shwartz-2011}
setting. In each round $t$, an algorithm chooses a point $w_t$ from a convex
\emph{decision set} $K$ and then receives a reward vector $g_t$. Algorithm's
goal is to keep its \emph{regret} small, defined as the difference between its
cumulative reward and the cumulative reward of a fixed strategy $u \in K$, that
is
\vspace{-.1cm}
\[
\Regret_T(u) = \sum_{t=1}^T \langle g_t, u \rangle - \sum_{t=1}^T \langle g_t, w_t \rangle \; .
\]
We focus on two particular sets, the $N$-dimensional probability simplex
$\Delta_N = \{ x \in \R^N ~:~ x \ge 0, \norm{x}_1 = 1\}$ and the Hilbert space
$\H$.  \ac{OLO} over $\Delta_N$ is referred to as the problem of \ac{LEA}.  We
assume bounds on the norms of the reward vectors: For \ac{OLO} over $\H$, we
assume that $\norm{g_t} \le 1$, and for \ac{LEA} we assume that $g_t \in
[0,1]^N$.

\vspace{-0.1cm}

\ac{OLO} is a basic building block of many machine learning problems. For
example, \ac{OCO}, the problem analogous to \ac{OLO} where $\langle g_t, u
\rangle$ is generalized to an arbitrary convex function $\ell_t(u)$, is solved
through a reduction to \ac{OLO}~\cite{Shalev-Shwartz-2011}.
\ac{LEA}~\cite{Littlestone-Warmuth-1994, Vovk-1998,
Cesa-Bianchi-Freund-Haussler-Helmbold-Schapire-Warmuth-1997} provides a way of
combining classifiers and it is at the heart of
boosting~\cite{Freund-Schapire-1997}. Batch and stochastic convex optimization
can also be solved through a reduction to \ac{OLO}~\cite{Shalev-Shwartz-2011}.

\vspace{-0.1cm}

To achieve optimal regret, most of the existing online algorithms require to
set the learning rate $\eta$ to an unknown/oracle value.  For example, to obtain the
optimal bound for \ac{OGD}, the learning rate has to be set with the knowledge
of the norm of the competitor $u$; first entry in Table~\ref{table:bounds}.
Likewise, the optimal learning rate for Hedge depends on the KL divergence
between the prior weighting $\pi$ and the unknown competitor $u$, fourth entry
in Table~\ref{table:bounds}.  Recently, new parameter-free
algorithms have been proposed, both for
\ac{LEA}~\cite{Chaudhuri-Freund-Hsu-2009, Chernov-Vovk-2010, Luo-Schapire-2014,
Luo-Schapire-2015, Koolen-van-Erven-2015, Foster-Rakhlin-Sridharan-2015} and
for \ac{OLO}/\ac{OCO} over Hilbert spaces~\cite{Streeter-McMahan-2012,
Orabona-2013, McMahan-Abernethy-2013, McMahan-Orabona-2014, Orabona-2014}.
These algorithms adapt to the number of experts and to the norm of the optimal
predictor, respectively, without the need to tune parameters. However, their
\emph{design and underlying intuition} is still a challenge.
\citet{Foster-Rakhlin-Sridharan-2015} proposed a unified framework, but it is not costructive.
Furthermore, all
existing algorithms for LEA either have sub-optimal regret bound (e.g. extra
$\scO(\log \log T)$ factor) or sub-optimal running time (e.g.  requiring
solving a numerical problem in every round, or with extra factors); see
Table~\ref{table:bounds}.

\begin{table}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{l c c c c}
\toprule
Algorithm & Worst-case regret guarantee& \begin{tabular}{@{}c@{}}Per-round time\\complexity\end{tabular} & Adaptive & \begin{tabular}{@{}c@{}}Unified \\  analysis\end{tabular}\\
\midrule
OGD, $\eta=\tfrac{1}{\sqrt{T}}$ \cite{Shalev-Shwartz-2011} & $\scO((1 + \norm{u}^2)\sqrt{T})$, $\forall u \in \H$ & $\scO(1)$ &  \\
OGD, $\eta=\tfrac{U}{\sqrt{T}}$ \cite{Shalev-Shwartz-2011} & $U \sqrt{T}$ for any $u \in \H$ s.t. $\norm{u} \le U$ & $\scO(1)$ &  \\
\cite{Orabona-2013} & $\scO(\norm{u} \ln(1+\norm{u}T) \sqrt{T})$, $\forall u \in \H$ & $\scO(1)$ & \checkmark \\
\cite{McMahan-Orabona-2014,Orabona-2014} & $\scO(\norm{u}\sqrt{T \ln(1+\norm{u}T)})$, $\forall u \in \H$ & $\scO(1)$ & \checkmark \\
This paper, Sec.~\ref{section:kt-olo} & $\scO(\norm{u}\sqrt{T \ln(1+\norm{u}T)})$, $\forall u \in \H$ & $\scO(1)$ & \checkmark & \checkmark\\
\midrule
Hedge~\cite{Freund-Schapire-1997}, $\eta=\tfrac{U}{\sqrt{T}}$ & $\scO(U\sqrt{T})$ for any $u \in \Delta_N$ s.t. $\sqrt{\KL{u}{\pi}} \le U$ & $\scO(N)$ &  \\
\cite{Chaudhuri-Freund-Hsu-2009}  & $\scO(\sqrt{T (1+\KL{u}{\pi})}+\ln^2 N)$, $\forall u \in \Delta_N$ & $\scO(N\,K)$\footnotemark[1]& \checkmark \\
\cite{Chernov-Vovk-2010} & $\scO(\sqrt{T \left(1+\KL{u}{\pi}\right)})$, $\forall u \in \Delta_N$ & $\scO(N\,K)$\footnotemark[1] & \checkmark \\
\cite{Chernov-Vovk-2010, Luo-Schapire-2015,Koolen-van-Erven-2015}\footnotemark[2] & $\scO(\sqrt{T \left(\ln \ln T+\KL{u}{\pi}\right)})$, $\forall u \in \Delta_N$ & $\scO(N)$ & \checkmark \\
\cite{Foster-Rakhlin-Sridharan-2015} & $\scO(\sqrt{T \left(1+\KL{u}{\pi}\right)})$, $\forall u \in \Delta_N$ & $\scO(N \ln \max_{u \in \Delta_N} \KL{u}{\pi})$\footnotemark[3] & \checkmark & \checkmark \\
This paper, Sec.~\ref{section:kt-lea} & $\scO(\sqrt{T \left(1+\KL{u}{\pi}\right)})$, $\forall u \in \Delta_N$ & $\scO(N)$ & \checkmark & \checkmark\\
\bottomrule
\end{tabular}}
\caption{\footnotesize{Algorithms for \ac{OLO} over Hilbert space and \ac{LEA}. We apologize for the big, but necessary, number of symbols used.
All of them are defined in Section~\ref{section:preliminaries}, while $\eta$ is the learning rate.}}
\label{table:bounds}
\end{table}
\footnotetext[1]{These algorithms require to solve a numerical problem at each step. The number $K$ is the number of steps needed to reach the required precision. Neither the precision nor $K$ are calculated in these papers.
%In general, $K$ is a function of the number rounds $T$ and the number of experts $N$.
}
\footnotetext[2]{The proof in \cite{Koolen-van-Erven-2015} can be modified to prove a KL bound, see \url{http://blog.wouterkoolen.info}.}
\footnotetext[3]{A variant of the algorithm in \cite{Foster-Rakhlin-Sridharan-2015} can be implemented with the stated time complexity~\cite{Foster:private}.}

\textbf{Contributions.} Instead of designing yet another algorithm, we show that a more fundamental notion subsumes
\emph{both} \ac{OLO} and \ac{LEA} parameter-free algorithms. We show that the
ability to maximize the wealth in bets on the outcomes of coin flips
\emph{implies} \ac{OLO} and \ac{LEA} parameter-free algorithms. We develop a
novel potential-based framework for betting algorithms and we instantiate it
with the Krichevsky-Trofimov estimator.  The resulting algorithms for \ac{OLO}
and \ac{LEA} are new, simple, and natural.  They also have optimal worst-case
guarantees on regret and time complexity; see Table~\ref{table:bounds}.
