\section{A Warm-Up: One-Dimensional Hilbert Space}
\label{section:one-dimensional-hilbert-space-olo}

There are many frameworks for the design and analysis of online learning algorithms, e.g. the potential function view~\cite{Cesa-BianchiL06}, the regularizer view~\cite{}, relax and randomize~\cite{}, etc. However, all of them do not provvide much help to craft the potentials/regularizer/relaxations/etc.
Here, we show how the betting view gives a very natural intuition on online learning problems and it also provides a clear path to the tools needed to solve them.

As a warm-up, let us consider an algorithm for OLO over one-dimensional Hilbert space $\R$.
Let $\{w_t\}_{t=1}^\infty$ be its sequence of predictions on a sequence of
rewards $\{g_t\}_{t=1}^\infty$, $g_t \in [-1,1]$. The total reward of the
algorithm after $t$ rounds is
\[
\Reward_t = \sum_{i=1}^t g_i w_i \; .
\]
Let us define ``wealth'' of the OLO algorithm as $\Wealth_t = \epsilon +
\Reward_t$ in accordance with \eqref{equation:reward-wealth}.
%Now, suppose we want to satisfy the recurrence
%\eqref{equation:wealth-recurrence}. Clearly, the recurrence is not necessarily
%satisfied for an arbitrary OLO algorithm.

Intuitevely, we want the reward to be big, on any sequence of $g_t$. We now restrict our attention to algorithms whose predictions are of the form of a bet:
\begin{equation}
\label{equation:one-dimensional-olo}
w_t = \beta_t \Wealth_{t-1}
= \beta_t \left(\epsilon+ \sum_{i=1}^{t-1} g_i w_i \right),
\end{equation}
where $\beta_t \in [-1,1]$. In this way the recurrence \eqref{equation:wealth-recurrence} holds.

%Indeed,
%\[
%\Wealth_t
%= g_t w_t + \Wealth_{t-1}
%= \beta_t \Wealth_{t-1} + \Wealth_{t-1}
%= (1+\beta_t) \Wealth_{t-1} \; .
%\]
This of course works in reverse: If we have a coin-betting algorithm that on a
sequence of coin flips $\{g_t\}_{t=1}^\infty$, $g_t \in [-1,1]$, bets fractions
$\beta_t \in [-1,1]$, we can use it to construct an OLO algorithm in a
one-dimensional Hilbert space $\R$ according to equation
\eqref{equation:one-dimensional-olo}.

Assume now that the betting algorithm at hand guarantees that its wealth is at least $F(\sum_{t=1}^T g_t)$ starting from an endowement $\epsilon$, then 
\begin{equation}
\label{equation:one-dimensional-olo-reward-lower-bound}
\Reward_T
= \sum_{t=1}^T g_t w_t
= \Wealth_T \ - \ \epsilon \ge F\left(\sum_{t=1}^T g_t \right) \ - \ \epsilon \; .
\end{equation}

We are almost done, we just need to convert a lower bound on the reward to an upper bound
on the regret. This can be done using the following lemma from~\cite{McMahanO14}.
\begin{lemma}[Reward-Regret relationship~\cite{McMahanO14}]
\label{lemma:reward-regret}
Let $V,V^*$ be a pair of dual vector spaces. Let $F:V \to \R \cup \{+\infty\}$
be a proper convex lower semi-continuous function and let $F^*:V^* \to \R \cup
\{+\infty\}$ be its Fenchel conjugate. Let $w_1, w_2, \dots, w_T \in V$ and
$g_1, g_2, \dots, g_T \in V^*$.  Then,
\[
\underbrace{\sum_{t=1}^T \langle g_t, w_t \rangle}_{\Reward_T} \ge F\left( \sum_{t=1}^T g_t \right)
\qquad \text{is equivalent to} \qquad
\forall u \in V^*, \quad
\underbrace{\sum_{t=1}^T \langle g_t, u - w_t\rangle}_{\Regret_T(u)} \le F^*(u) \; .
\]
\end{lemma}

Applying the lemma, we get a regret upper bound
\[
\forall u \in \H \qquad \qquad
\Regret_T(u) \le F^*(u) \ + \ \epsilon \; .
\]

Hence, we have showed that if we have a betting algorithm that guarantess a minimum wealth, this can be used to design and analyze the 1-d \ac{OLO} case. Moreover, the faster is the growth of the wealth, the smaller the regret will be.
In the next sections we will provvide the tools and the reductions to extend this simple case to the generic \ac{OLO} case and to \ac{LEA} as well.