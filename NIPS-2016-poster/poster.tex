\documentclass[largefonts,landscape]{sciposter}

\usepackage{pstricks}
\usepackage[pdftex]{graphicx}
\usepackage{multirow,array}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{algorithmic}
\usepackage{fancybox}
\usepackage{multicol}
\usepackage{natbib}

\definecolor{SectionCol}{rgb}{1.0,1.0,1.0}    %% section heading color
\definecolor{BoxCol}{rgb}{0.5,0.0,0.0}

\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{definition}{Definition}
\newtheorem*{conjecture}{Conjecture}

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Reward}{Reward}
\DeclareMathOperator{\Wealth}{Wealth}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\R}{\mathbb{R}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\indicator}{\mathbf{1}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\KL}[2]{\mathrm{KL}\left({#1} \, \middle\| \, {#2}\right)}
\newcommand{\grad}{\nabla}
\newcommand{\Breg}{\mathcal{B}}

\begin{document}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\footlogo}{Typeset by pdf\LaTeX}

\title{Parameter-Free and Scale-Free Online Algorithms}
\author{Francesco Orabona\footnotemark[1], D\'avid P\'al\footnotemark[2]}
\institute{%
\footnotemark[1] Stony Brook University, NY\\
\footnotemark[2] Yahoo Research, New York, NY}

\date{November 9, 2016}

\leftlogo[1.2]{stony-brook-logo}
\rightlogo[1.2]{yahoo-logo}
\conference{Theoretical Foundations for Learning from Easy Data, November 7--11, 2016, Lorentz Center, University of Leiden, Leiden, Netherlands}

\maketitle

\setlength{\parindent}{0em}
\setlength{\columnsep}{4cm}
\begin{multicols}{3}

\section*{Online Learning}

Non-empty closed convex \emph{decision set} $K \subseteq V$.

\vspace{1cm}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Online Linear Optimization
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\FOR{$t=1,2,3,\dots$}
\STATE Pick $w_t \in K$
\STATE Receive $\ell_t \in V^*$
\STATE Suffer loss $\langle \ell_t, w_t \rangle$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

\vspace{1cm}

$$
\Regret_T(u) = \sum_{t=1}^T \langle \ell_t, w_t \rangle - \sum_{t=1}^T \langle \ell_t, u \rangle \; .
$$

\vspace{1cm}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Follow The Regularized Leader (FTRL)
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE Initialize $L_0 \leftarrow 0$
\FOR{$t=1,2,3,\dots$}
\STATE Choose a regularizer $R_t:K \to \R$
\STATE $w_t \leftarrow \argmin_{w \in K} \langle L_{t-1}, w \rangle + R_t(w)$
\STATE Predict $w_t$
\STATE Observe $\ell_t \in V^*$
\STATE $L_t \leftarrow L_{t-1} + \ell_t$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

\vspace{1cm}

$$
\Regret_T(u) \le R_{T+1}(u) + R_1^*(0) + \sum_{t=1}^T \Breg_{R_t^*}(-L_t, -L_{t-1}) - R_t^*(-L_t) + R_{t+1}^*(-L_t)
$$

\vspace{1cm}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Mirror Descent (MD)
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE Choose a regularizer $R_0:K \to \R$
\STATE $w_1 \leftarrow \argmin_{w \in K} R_0(w)$
\FOR{$t=1,2,3,\dots$}
\STATE Predict $w_t$
\STATE Observe $\ell_t \in V^*$
\STATE Choose a regularizer $R_t:K \to \R$
\STATE $w_{t+1} \leftarrow \argmin_{w \in K} \langle \ell_t, w \rangle + \Breg_{R_t}(w, w_t)$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

\vspace{1cm}

$$
\Regret_T(u) \le \sum_{t=1}^T \langle \ell_t, w_{t} - w_{t+1} \rangle - \Breg_{R_t}(w_{t+1}, w_t) + \Breg_{R_t}(u, w_{t}) - \Breg_{R_t}(u, w_{t+1})
$$


\columnbreak

\section*{Scale-Free Algorithms}

\begin{minipage}{0.8\linewidth}
Non-negative $\lambda$-strongly convex $R:K \to \R$ w.r.t. $\norm{\cdot}$.
\begin{center}
\def\arraystretch{2}%  1 is the default, change to whatever you need
\everymath{\displaystyle}
\begin{tabular}{c|c}
FTRL & MD \\ \hline
$R_t(u) = \frac{R(u)}{\sqrt{\sum_{s=1}^{t-1} \norm{\ell_s}_*^2}}$ \hspace{1cm} & \hspace{1cm}  $R_t(u) = \frac{R(u)}{\sqrt{\sum_{s=1}^t \norm{\ell_s}_*^2}}$ \\
\end{tabular}
\end{center}
\end{minipage}

\begin{minipage}{1.0\linewidth}
FTRL:
$$
\Regret_T(u) \le \left(\frac{2.75}{\lambda} + R(u)\right) \sqrt{\sum_{t=1}^T \norm{\ell_t}_*^2} + 3.5 \min \left\{ D, \frac{\sqrt{T-1}}{\lambda} \right\} \max_{t \le T} \norm{\ell_t}_*
$$

MD:
$$
\Regret_T(u) \le \left(\frac{1}{\lambda} + \sup_{v \in K} \Breg_{R}(u,v) \right) \sqrt{\sum_{t=1}^T \norm{\ell_t}_*^2}
$$

If $f$ is $1$-strongly convex and $R(u) = \lambda f(u)$, optimizing $\lambda$ gives:
\begin{align*}
\text{FTRL:} \qquad \Regret_T & \le 13.3 \sqrt{ \sup_{v \in K} f(v) \sum_{t=1}^T \norm{\ell_t}_*^2} \\
\text{MD:} \qquad \Regret_T & \le 2 \sqrt{ \sup_{u,v \in K} \Breg_{f}(u,v) \sum_{t=1}^T \norm{\ell_t}_*^2}
\end{align*}
\end{minipage}

\vspace{0.6cm}

\hrule height 4pt

\vspace{0.6cm}


\begin{minipage}{1.0\linewidth}
MD bound is much worse than FTRL bound:

Let $v^* = \argmin_{v \in K} R(v)$.
WLOG assume $R(v^*) = 0$ and $\grad R(v^*) = 0$.

\begin{itemize}
\item $R(u) \le \sup_{v \in K} \Breg_{R}(u,v)$
\item $R(u)$ is finite
\item There are regularizers for which $\sup_{v \in K} \Breg_{R}(u,v) = + \infty$.
\end{itemize}
\end{minipage}

\vspace{0.6cm}

\hrule height 1pt

\vspace{0.6cm}

\begin{minipage}{1.0\linewidth}
If $\sup_{v \in K} \Breg_{R}(u,v) = +\infty$, regret explodes.
Mirror Descent on $K=\R$ with regularizer $R_t(u) = \frac{u^2}{2 \sqrt{\sum_{i=1}^t \ell_i^2}}$
is the same as Gradient Descent with step size $1/\sqrt{t}$ on $K=\R$.
On the input
$$
(\ell_1, \ell_2, \dots, \ell_T) = (\underbrace{-1, -1, \dots, -1}_{T/2}, \underbrace{+1, +1, \dots, +1}_{T/2})
$$
regret with respect to $u=0$ is $\Omega(T^{1.5})$.
\end{minipage}

\vspace{0.6cm}

\hrule height 1pt

\vspace{0.6cm}

\begin{minipage}{1.0\linewidth}
Mirror Descent on $K=\Delta_N$ with regularizer $R_t(u) = \frac{\KL{u}{\frac{\textbf{1}}{N}}}{\sqrt{\sum_{i=1}^t \norm{\ell_i}_\infty^2}}$
on the input
$$
(\ell_1, \ell_2, \dots, \ell_T) = (\underbrace{-e_1, -e_1, \dots, -e_1}_{T/3}, \underbrace{-e_2, -e_2, \dots, -e_2}_{2T/3})
$$
regret is $\Omega(T)$.
\end{minipage}

\columnbreak

\section*{Parameter-Free Algorithms for Hilbert Spaces}

Let $\H$ be a Hilbert space. Let $K=\H$ and assume $\norm{\ell_t} \le 1$.
Gradient Descent with (constant) learning rate $1/\sqrt{T}$ satisfies
$$
\forall u \in \H \qquad \Regret_T(u) \le \frac{1}{2}\left(1 + \norm{u}^2\right) \sqrt{T} \; .
$$
With learning rate $D/\sqrt{T}$, it satisfies
$$
\forall u : \norm{u} \le D  \qquad \Regret_T(u) \le D \sqrt{T} \; .
$$
Can we get
$$
\forall u \in \H \qquad \Regret_T(u) \le \norm{u} \sqrt{T} \; ?
$$

\vspace{1cm}

\begin{lemma}
Let $\Reward_T = - \sum_{t=1}^T \langle \ell_t, w_t \rangle$.
Let $F:K \to \R$ be convex.
$$
\big( \forall u \in K \quad \Regret_T(u) \le F(u) \big)
\quad \text{if and only if} \quad
\Reward_T \ge F^*\left(-\sum_{t=1}^T \ell_t\right) \; .
$$
\end{lemma}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Coin Betting
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE $\Wealth_0 \leftarrow 1$
\FOR{$t=1,2,3,\dots$}
\STATE $\beta_t \leftarrow \frac{1}{t} \sum_{i=1}^{t-1} c_i$ \quad (KT estimator)
\STATE Bet $w_t = \beta_t \Wealth_{t-1}$
\STATE Observe coin flip $c_t \in \{+1, -1\}$
\STATE $\Wealth_t \leftarrow \Wealth_{t-1} \ + \ c_t \cdot w_t$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

Krichevsky-Trofimov showed that
$$
\Wealth_T \ge \exp\left( T \cdot \KL{\frac{1}{2} + \frac{\sum_{t=1}^T c_t}{2T} }{\frac{1}{2}} \right)
$$

\vspace{0.5cm}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Coin Betting for Hilbert Spaces
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE $L_0 \leftarrow 0$
\STATE $\Wealth_0 \leftarrow 1$
\FOR{$t=1,2,3,\dots$}
\STATE Predict $w_t = - \frac{\Wealth_{t-1}}{t}  L_{t-1}$
\STATE Observe $\ell_t \in \H$ (assume $\norm{\ell_t} \le 1$)
\STATE $L_t \leftarrow L_{t-1} + \ell_t$
\STATE $\Wealth_t \leftarrow \Wealth_{t-1} - \langle \ell_t, w_t \rangle$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

Generalizing Krichevsky-Trofimov,
$$
1 + \Reward_T = \Wealth_T \ge \exp\left( T \cdot \KL{\frac{1}{2} + \frac{\norm{\sum_{t=1}^T \ell_t}}{2T} }{\frac{1}{2}} \right)
$$
Using the Lemma, we get $\Regret_T(u) \le \norm{u} \sqrt{T \ln(1 + 24 T^2 \norm{u}^2)} + 1$.

\end{multicols}

\end{document}
