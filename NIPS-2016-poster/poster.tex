\documentclass[largefonts,landscape]{sciposter}

\usepackage{pstricks}
\usepackage[pdftex]{graphicx}
\usepackage{multirow,array}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{algorithmic}
\usepackage{fancybox}
\usepackage{multicol}
\usepackage{natbib}

\definecolor{SectionCol}{rgb}{1.0,1.0,1.0}    %% section heading color
\definecolor{BoxCol}{rgb}{0.5,0.0,0.0}

\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{definition}{Definition}
\newtheorem*{conjecture}{Conjecture}

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Reward}{Reward}
\DeclareMathOperator{\Wealth}{Wealth}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\R}{\mathbb{R}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\indicator}{\mathbf{1}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\KL}[2]{\mathrm{KL}\left({#1} \, \middle\| \, {#2}\right)}
\newcommand{\grad}{\nabla}
\newcommand{\Breg}{\mathcal{B}}

\begin{document}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\footlogo}{Typeset by pdf\LaTeX}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}

\title{Coin Betting and Parameter-Free Online Learning}
\author{Francesco Orabona\footnotemark[1], D\'avid P\'al\footnotemark[2]}
\institute{%
\footnotemark[1] Stony Brook University, NY\\
\footnotemark[2] Yahoo Research, New York, NY}

\date{December 5, 2016}

\leftlogo[1.2]{stony-brook-logo}
\rightlogo[1.2]{yahoo-logo}
\conference{The $30^{\text{th}}$ Annual Conference on Neural Information Processing Systems (NIPS), December 5--10, 2016, Barcelona, Spain}

\maketitle

\setlength{\parindent}{0em}
\setlength{\columnsep}{4cm}
\begin{multicols}{3}

\section*{Are you still tuning hyperparameters?}

Regularized empirical risk minimization:
\begin{equation}
\label{equation:objective-function}
    \argmin_{w \in \R^d} \ \frac{\lambda}{2} \norm{w}^2 + \sum_{i=1}^T f(w, x_i, y_i)
\end{equation}
where $f$ is convex in $w$.
\begin{itemize}
\item How do you choose the regularizer weight $\lambda$?
\end{itemize}

\vspace{1cm}

Stochastic approximation:
\begin{equation}
\label{equation:objective-function-sa}
    w_t = w_{t-1} - \eta_t \grad f(w_{t-1}, x_t, y_t)
\end{equation}
where $f$ is convex in $w$.
\begin{itemize}
\item How do you choose the learning rate $\eta_t$?
\end{itemize}

\vspace{1cm}

\begin{itemize}
\item Why is the algorithm not able to select $\lambda$ and/or $\eta_t$ automatically?
\end{itemize}

\section*{Online Learning}

Let $V,V^*$ be a pair of dual vector spaces.

\vspace{1cm}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Online Linear Optimization
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\REQUIRE{Non-empty closed convex \emph{decision set} $K \subseteq V$}
\FOR{$t=1,2,3,\dots$}
\STATE Pick $w_t \in K$
\STATE Receive reward vector $g_t \in V^*$
\STATE Gain reward $\langle g_t, w_t \rangle$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

\vspace{0.5cm}

$$
\Regret_T(u) = \sum_{t=1}^T \langle g_t, u \rangle - \sum_{t=1}^T \langle g_t, w_t \rangle\; .
$$

\vspace{0.5cm}

\begin{itemize}
\item Online Gradient Ascent with learning rate $\eta$ satisfies
\[
\forall u \in V \qquad \Regret_T(u) \le \tfrac{\norm{u}^2}{2\eta} + \tfrac{\eta}{2} \sum_{t=1}^T \norm{g_t}^2 \; .
\]
\item Optimal oracle choice: $\eta = \frac{\norm{u}}{\sqrt{\sum_{t=1}^T \norm{g_t}^2}}$.
\item Many algorithms adapt to the norms of the gradients (e.g. AdaGrad) while neglecting dependency on $\norm{u}$.
\item Adapting to $u$ is \emph{more difficult and more important}.
\item Better guarantees are indeed possible: Streeter\&McMahan (2012), Orabona (2013), McMahan\&Abernethy (2013), McMahan\&Orabona (2014), Orabona (2014)
\[
\forall u \in \H \qquad \Regret_T(u) \le \big(O(1)+\polylog(1 + \norm{u})\norm{u} \big) \sqrt{T} \; .
\]
\end{itemize}

\columnbreak

\section*{Parameter-Free Algorithms for Hilbert Spaces}

Let $\H$ be a Hilbert space. Let $K=\H$ and assume $\norm{g_t} \le 1$.
Gradient Ascent with (constant) learning rate $1/\sqrt{T}$ satisfies
$$
\forall u \in \H \qquad \Regret_T(u) \le \frac{1}{2}\left(1 + \norm{u}^2\right) \sqrt{T} \; .
$$
With learning rate $D/\sqrt{T}$, it satisfies
$$
\forall u : \norm{u} \le D  \qquad \Regret_T(u) \le D \sqrt{T} \; .
$$
Can we get
$$
\forall u \in \H \qquad \Regret_T(u) \le \norm{u} \sqrt{T} \; ?
$$

\vspace{1cm}

\begin{lemma}
Let $\Reward_T = \sum_{t=1}^T \langle g_t, w_t \rangle$.
Let $F:K \to \R$ be convex.
$$
\big( \forall u \in K \quad \Regret_T(u) \le F(u) \big)
\quad \text{if and only if} \quad
\Reward_T \ge F^*\left(\sum_{t=1}^T g_t\right) \; .
$$
\end{lemma}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Coin Betting
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE $\Wealth_0 \leftarrow 1$
\FOR{$t=1,2,3,\dots$}
\STATE $\beta_t \leftarrow \frac{1}{t} \sum_{i=1}^{t-1} c_i$ \quad (KT estimator)
\STATE Bet $w_t = \beta_t \Wealth_{t-1}$
\STATE Observe coin flip $c_t \in \{+1, -1\}$
\STATE $\Wealth_t \leftarrow \Wealth_{t-1} \ + \ c_t \cdot w_t$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

Krichevsky-Trofimov showed that
$$
\Wealth_T \ge \exp\left( T \cdot \KL{\frac{1}{2} + \frac{\sum_{t=1}^T c_t}{2T} }{\frac{1}{2}} \right)
$$

\vspace{0.5cm}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Coin Betting for Hilbert Spaces
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE $L_0 \leftarrow 0$
\STATE $\Wealth_0 \leftarrow 1$
\FOR{$t=1,2,3,\dots$}
\STATE Predict $w_t = \frac{\Wealth_{t-1}}{t} \,  G_{t-1}$
\STATE Observe $g_t \in \H$ such that $\norm{g_t} \le 1$
\STATE $G_t \leftarrow G_{t-1} + g_t$
\STATE $\Wealth_t \leftarrow \Wealth_{t-1} \ + \ \langle g_t, w_t \rangle$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

Generalizing Krichevsky-Trofimov,
$$
1 + \Reward_T = \Wealth_T \ge \exp\left( T \cdot \KL{\frac{1}{2} + \frac{\norm{\sum_{t=1}^T g_t}}{2T} }{\frac{1}{2}} \right)
$$
Using the Lemma, we get $\Regret_T(u) \le \norm{u} \sqrt{T \ln(1 + 24 T^2 \norm{u}^2)} + 1$.


\columnbreak

\section*{Parameter-Free Algorithms for Learning with Expert Advice}

Let $K=\Delta_N$ and assume $g_t \in [0,1]^N$.
Hedge with (constant) learning rate $\eta$ satisfies
\[
\forall u \in \Delta_N \qquad
\Regret_T(u) \leq O \left(\tfrac{\KL{u}{\pi}}{\eta} + \eta T \right)
\]
With learning rate $\eta = D/\sqrt{T}$, it satisfies
$$
\forall u : \KL{u}{\pi} \le D  \qquad \Regret_T(u) \le \sqrt{T \cdot D} \; .
$$
Can we get
$$
\forall u \in \Delta_N \qquad \Regret_T(u) \le \sqrt{T \cdot \KL{u}{\pi}} \; ?
$$

\vspace{1cm}

\begin{center}
Learning with Expert Advice
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\REQUIRE{Number of experts $N$}
\REQUIRE{Prior distribution $\pi \in \Delta_N$}
\REQUIRE{Number of rounds $T$}
\FOR{$t=1,2,\dots,T$}
\STATE{For each $i \in [N]$, set $w_{t,i} \leftarrow \tfrac{\sum_{j=1}^{t-1} \widetilde g_{j,i}}{t+T/2} \left(1 + \sum_{j=1}^{t-1} \widetilde g_{j,i} w_{j,i} \right)$}
\STATE{For each $i \in [N]$, set $\widehat{p}_{t,i} \leftarrow \pi_i [w_{t,i}]_+$}
\STATE{Predict with $p_t \leftarrow
\begin{cases}
\widehat{p}_t/\norm{\widehat{p_t}}_1 & \text{if $\norm{\widehat p_t}_1 > 0$} \\
\pi & \text{if $\norm{\widehat p_t}_1 = 0$}
\end{cases}$}
\STATE{Receive reward vector $g_t \in [0,1]^N$}
\STATE{For each $i \in [N]$, set $\widetilde g_{t,i} \leftarrow \begin{cases}
g_{t,i} - \langle g_t, p_t \rangle & \text{if $w_{t,i} > 0$} \\
[g_{t,i} - \langle g_t, p_t \rangle]_+ & \text{if $w_{t,i} \le 0$}
\end{cases}$}
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

\vspace{1cm}

\begin{theorem}
Algorithm with input $N,\pi,T$
for any rewards vectors $g_1, g_2, \dots, g_T \in [0,1]^N$ satisfies
\[
\forall u \in \Delta_N \qquad \qquad \Regret_T(u) \le \sqrt{3T (3 + \KL{u}{\pi})} \; .
\]
\end{theorem}


\end{multicols}

\end{document}
