\documentclass[largefonts,landscape]{sciposter}

\usepackage{pstricks}
\usepackage[pdftex]{graphicx}
\usepackage{multirow,array}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{algorithmic}
\usepackage{fancybox}
\usepackage{multicol}
\usepackage{natbib}

\definecolor{SectionCol}{rgb}{1.0,1.0,1.0}    %% section heading color
\definecolor{BoxCol}{rgb}{0.5,0.0,0.0}

\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{definition}{Definition}
\newtheorem*{conjecture}{Conjecture}

\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Reward}{Reward}
\DeclareMathOperator{\Wealth}{Wealth}
\DeclareMathOperator{\polylog}{polylog}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\R}{\mathbb{R}}
\renewcommand{\H}{\mathcal{H}}
\newcommand{\indicator}{\mathbf{1}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\KL}[2]{\mathrm{KL}\left({#1} \, \middle\| \, {#2}\right)}
\newcommand{\grad}{\nabla}
\newcommand{\Breg}{\mathcal{B}}

\begin{document}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\renewcommand{\footlogo}{Typeset by pdf\LaTeX}

\title{Parameter-Free and Scale-Free Online Algorithms}
\author{Francesco Orabona\footnotemark[1], D\'avid P\'al\footnotemark[2]}
\institute{%
\footnotemark[1] Stony Brook University, NY\\
\footnotemark[2] Yahoo Research, New York, NY}

\date{November 9, 2016}

\leftlogo[1.2]{stony-brook-logo}
\rightlogo[1.2]{yahoo-logo}
\conference{Theoretical Foundations for Learning from Easy Data, November 7--11, 2016, Lorentz Center, University of Leiden, Leiden, Netherlands}

\maketitle

\setlength{\parindent}{0em}
\setlength{\columnsep}{4cm}
\begin{multicols}{3}

\section*{Are you still tuning hyperparameters?}

Regularized empirical risk minimization:
\begin{equation}
\label{equation:objective-function}
    \argmin_{w \in \R^d} \ \frac{\lambda}{2} \norm{w}^2 + \sum_{i=1}^N f(w, x_i, y_i)
\end{equation}
where $f$ is convex in $w$.
\begin{itemize}
\item How do you choose the regularizer weight $\lambda$?
\end{itemize}

\vspace{1cm}

Stochastic approximation:
\begin{equation}
\label{equation:objective-function-sa}
    w_t = w_{t-1} - \eta_t \grad f(w_{t-1}, x_t, y_t)
\end{equation}
where $f$ is convex in $w$.
\begin{itemize}
\item How do you choose the learning rate $\eta_t$?
\end{itemize}

\vspace{1cm}

\begin{itemize}
\item Why is the algorithm not able to select $\lambda$ and/or $\eta_t$ automatically?
\end{itemize}

\section*{Online Learning}


\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Online Linear Optimization
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE{Input: Non-empty closed convex \emph{decision set} $K \subseteq V$}
\FOR{$t=1,2,3,\dots$}
\STATE Pick $w_t \in K$
\STATE Receive $g_t \in V^*$
\STATE Gain $\langle g_t, w_t \rangle$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

\vspace{1cm}

$$
\Regret_T(u) = \sum_{t=1}^T \langle g_t, u \rangle - \sum_{t=1}^T \langle g_t, w_t \rangle\; .
$$

\begin{itemize}
\item OGD with learning rate $\eta$ satisfies
\[
\forall u \in \H \qquad \Regret_T(u) \le \tfrac{\norm{u}^2}{2\eta} + \tfrac{\eta}{2} \sum_{t=1}^T \norm{\ell_t}^2 \; .
\]
\item Optimal oracle choice: $\eta = \frac{\norm{u}}{\sqrt{\sum_{t=1}^T \norm{\ell_t}^2}}$.
\item Many algorithms adapt to the norms of the gradients (e.g. AdaGrad) while neglecting dependency on $\norm{u}$.
\item Adapting to $u$ is \emph{more difficult and more important}.
\item Better guarantees are indeed possible: Streeter\&McMahan (2012), Orabona (2013), McMahan\&Abernethy (2013), McMahan\&Orabona (2014), Orabona (2014)
\[
\forall u \in \H \qquad \Regret_T(u) \le \big(O(1)+\polylog(1 + \norm{u})\norm{u} \big) \sqrt{T} \; .
\]
\end{itemize}

\columnbreak

 
% \section*{Scale-Free Algorithms}
% 
% \begin{minipage}{0.8\linewidth}
% Non-negative $\lambda$-strongly convex $R:K \to \R$ w.r.t. $\norm{\cdot}$.
% \begin{center}
% \def\arraystretch{2}%  1 is the default, change to whatever you need
% \everymath{\displaystyle}
% \begin{tabular}{c|c}
% FTRL & MD \\ \hline
% $R_t(u) = \frac{R(u)}{\sqrt{\sum_{s=1}^{t-1} \norm{\ell_s}_*^2}}$ \hspace{1cm} & \hspace{1cm}  $R_t(u) = \frac{R(u)}{\sqrt{\sum_{s=1}^t \norm{\ell_s}_*^2}}$ \\
% \end{tabular}
% \end{center}
% \end{minipage}
% 
% \begin{minipage}{1.0\linewidth}
% FTRL:
% $$
% \Regret_T(u) \le \left(\frac{2.75}{\lambda} + R(u)\right) \sqrt{\sum_{t=1}^T \norm{\ell_t}_*^2} + 3.5 \min \left\{ D, \frac{\sqrt{T-1}}{\lambda} \right\} \max_{t \le T} \norm{\ell_t}_*
% $$
% 
% MD:
% $$
% \Regret_T(u) \le \left(\frac{1}{\lambda} + \sup_{v \in K} \Breg_{R}(u,v) \right) \sqrt{\sum_{t=1}^T \norm{\ell_t}_*^2}
% $$
% 
% If $f$ is $1$-strongly convex and $R(u) = \lambda f(u)$, optimizing $\lambda$ gives:
% \begin{align*}
% \text{FTRL:} \qquad \Regret_T & \le 13.3 \sqrt{ \sup_{v \in K} f(v) \sum_{t=1}^T \norm{\ell_t}_*^2} \\
% \text{MD:} \qquad \Regret_T & \le 2 \sqrt{ \sup_{u,v \in K} \Breg_{f}(u,v) \sum_{t=1}^T \norm{\ell_t}_*^2}
% \end{align*}
% \end{minipage}
% 
% \vspace{0.6cm}
% 
% \hrule height 4pt
% 
% \vspace{0.6cm}
% 
% 
% \begin{minipage}{1.0\linewidth}
% MD bound is much worse than FTRL bound:
% 
% Let $v^* = \argmin_{v \in K} R(v)$.
% WLOG assume $R(v^*) = 0$ and $\grad R(v^*) = 0$.
% 
% \begin{itemize}
% \item $R(u) \le \sup_{v \in K} \Breg_{R}(u,v)$
% \item $R(u)$ is finite
% \item There are regularizers for which $\sup_{v \in K} \Breg_{R}(u,v) = + \infty$.
% \end{itemize}
% \end{minipage}
% 
% \vspace{0.6cm}
% 
% \hrule height 1pt
% 
% \vspace{0.6cm}
% 
% \begin{minipage}{1.0\linewidth}
% If $\sup_{v \in K} \Breg_{R}(u,v) = +\infty$, regret explodes.
% Mirror Descent on $K=\R$ with regularizer $R_t(u) = \frac{u^2}{2 \sqrt{\sum_{i=1}^t \ell_i^2}}$
% is the same as Gradient Descent with step size $1/\sqrt{t}$ on $K=\R$.
% On the input
% $$
% (\ell_1, \ell_2, \dots, \ell_T) = (\underbrace{-1, -1, \dots, -1}_{T/2}, \underbrace{+1, +1, \dots, +1}_{T/2})
% $$
% regret with respect to $u=0$ is $\Omega(T^{1.5})$.
% \end{minipage}
% 
% \vspace{0.6cm}
% 
% \hrule height 1pt
% 
% \vspace{0.6cm}
% 
% \begin{minipage}{1.0\linewidth}
% Mirror Descent on $K=\Delta_N$ with regularizer $R_t(u) = \frac{\KL{u}{\frac{\textbf{1}}{N}}}{\sqrt{\sum_{i=1}^t \norm{\ell_i}_\infty^2}}$
% on the input
% $$
% (\ell_1, \ell_2, \dots, \ell_T) = (\underbrace{-e_1, -e_1, \dots, -e_1}_{T/3}, \underbrace{-e_2, -e_2, \dots, -e_2}_{2T/3})
% $$
% regret is $\Omega(T)$.
% \end{minipage}
% 
% \columnbreak

\section*{Parameter-Free Algorithms for Hilbert Spaces}

Let $\H$ be a Hilbert space. Let $K=\H$ and assume $\norm{g_t} \le 1$.
Gradient Descent with (constant) learning rate $1/\sqrt{T}$ satisfies
$$
\forall u \in \H \qquad \Regret_T(u) \le \frac{1}{2}\left(1 + \norm{u}^2\right) \sqrt{T} \; .
$$
With learning rate $D/\sqrt{T}$, it satisfies
$$
\forall u : \norm{u} \le D  \qquad \Regret_T(u) \le D \sqrt{T} \; .
$$
Can we get
$$
\forall u \in \H \qquad \Regret_T(u) \le \norm{u} \sqrt{T} \; ?
$$

\vspace{1cm}

\begin{lemma}
Let $\Reward_T = \sum_{t=1}^T \langle g_t, w_t \rangle$.
Let $F:K \to \R$ be convex.
$$
\big( \forall u \in K \quad \Regret_T(u) \le F(u) \big)
\quad \text{if and only if} \quad
\Reward_T \ge F^*\left(\sum_{t=1}^T g_t\right) \; .
$$
\end{lemma}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Coin Betting
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE $\Wealth_0 \leftarrow 1$
\FOR{$t=1,2,3,\dots$}
\STATE $\beta_t \leftarrow \frac{1}{t} \sum_{i=1}^{t-1} c_i$ \quad (KT estimator)
\STATE Bet $w_t = \beta_t \Wealth_{t-1}$
\STATE Observe coin flip $c_t \in \{+1, -1\}$
\STATE $\Wealth_t \leftarrow \Wealth_{t-1} \ + \ c_t \cdot w_t$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

Krichevsky-Trofimov showed that
$$
\Wealth_T \ge \exp\left( T \cdot \KL{\frac{1}{2} + \frac{\sum_{t=1}^T c_t}{2T} }{\frac{1}{2}} \right)
$$

\vspace{0.5cm}

\setlength{\fboxrule}{5pt}
\setlength{\fboxsep}{10pt}
\begin{center}
Coin Betting for Hilbert Spaces
\vspace{0.3cm}

\colorbox[rgb]{0.80,0.55,0.98}{\fbox{
\begin{minipage}{0.8\linewidth}
\begin{algorithmic}
{
\STATE $L_0 \leftarrow 0$
\STATE $\Wealth_0 \leftarrow 1$
\FOR{$t=1,2,3,\dots$}
\STATE Predict $w_t = \frac{\Wealth_{t-1}}{t}  L_{t-1}$
\STATE Observe $g_t \in \H$ (assume $\norm{g_t} \le 1$)
\STATE $L_t \leftarrow L_{t-1} + g_t$
\STATE $\Wealth_t \leftarrow \Wealth_{t-1} + \langle g_t, w_t \rangle$
\ENDFOR
}
\end{algorithmic}
\end{minipage}
}}
\end{center}

Generalizing Krichevsky-Trofimov,
$$
1 + \Reward_T = \Wealth_T \ge \exp\left( T \cdot \KL{\frac{1}{2} + \frac{\norm{\sum_{t=1}^T g_t}}{2T} }{\frac{1}{2}} \right)
$$
Using the Lemma, we get $\Regret_T(u) \le \norm{u} \sqrt{T \ln(1 + 24 T^2 \norm{u}^2)} + 1$.


\columnbreak

\section*{Parameter-Free Algorithms for Learning with Experts' Advice}

Let $K=\Delta_N$ and assume $g_t \in [0,1]^N$.
Hedge with (constant) learning rate $1/\sqrt{T}$ satisfies
\[
\Regret_T(u) \leq O(\tfrac{KL(u||\pi)}{\eta} + \eta T), \quad \forall u \in \Delta_N
\]
With learning rate $D/\sqrt{T}$, it satisfies
$$
\forall u : KL(u||\pi) \le D  \qquad \Regret_T(u) \le D \sqrt{T} \; .
$$
Can we get
$$
\forall u \in \Delta_N \qquad \Regret_T(u) \le \sqrt{T \, KL(u||\pi)} \; ?
$$

\vspace{1cm}


\end{multicols}

\end{document}
