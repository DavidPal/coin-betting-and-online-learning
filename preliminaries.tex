\section{Setting and Notation}

The Kullback-Leibler divergence between two discrete
distributions $p$ and $q$ is
$$
\KL{p}{q} = \sum_{i} p_i \ln\left( \frac{p_i}{q_i} \right) \; ,
$$
where the sum is over all possible outcomes. Abusing notation, if $p,q$ are real
numbers in $[0,1]$, we denote by
$$
\KL{p}{q} = p \ln \left(\frac{p}{q} \right) + (1-p) \ln \left(\frac{1-p}{1-q} \right) \; .
$$
the Kullback-Leibler divergence between Bernoulli distributions with parameters
$p$ and $q$.

We denote by $\H$ a Hilbert space, by $\langle \cdot, \cdot\rangle$ its
associated inner product, and by $\norm{\cdot}$ the norm induced by the dot
product. We denote by $\|\cdot\|_1$ the $1$-norm in $\R^N$, that is, $\|x\|_1 =
\sum_{i=1}^N x_i$.

A function $F:I \to \R$ defined on an interval $I$ is called
\emph{logarithmically convex} if $f(x) = \ln(F(x))$ is convex. In particular,
$F(x)$ must be positive. Note that if $F(x)$ logarithmically convex, then it is
also convex (in the usual sense), since $F(x) = \exp(f(x))$ is a composition
of a convex function $f(x)$ and an increasing convex function $\exp(\cdot)$.

Let $f:V \to \R$ be a function defined on vector space $V$.
Fenchel conjugate of $f$ is a function $f^*:V^* \to \R \cup \{\infty\}$
defined on the dual vector space $V^*$ by
$$
f^*(\theta) = \sup_{x \in V} \langle \theta, x \rangle - f(x) \; .
$$
If $f$ is lower semi-continous and convex, then $f^*$ is also is lower
semi-continous and convex, and furthermore the Fenchel conjugate of $f^*$ is
$f$.

\subsection{Online Linear Optimization over a Hilbert Space}

Let $\H$ be a Hilbert space. \textsc{Online Linear Optimization over a Hilbert
Space $\H$} is an online problem where in each round $t$, an algorithm chooses a
point $w_t \in \H$ and then receives a reward vector $g_t \in \H$. Algorithm's
instantenous reward in round $t$ is $\langle g_t, w_t \rangle$. The aim of the
algorithm is to minimize its \emph{regret} after $T$ rounds, that is, the
difference between its cumulative reward $\sum_{t=1}^n \langle g_t, w_t \rangle$
and the cumulative reward $\sum_{t=1}^T \langle g_t, u \rangle$ of the of a
hypothetical strategy (\emph{competitor}) that would choose the same point $u
\in \H$ in every round. Formally, regret of the algorithm after $T$ rounds with
respect to a competitor $u \in \H$ is defined as
\begin{equation}
\label{equation:regret-definition}
\Regret_T(u) = \sum_{t=1}^T \langle g_t , u - w_t \rangle \; .
\end{equation}
In this paper we make the assumption that $\|g_t\| \le 1$.


\subsection{Learning with Expert Advice}

Let $N \ge 2$ be a positive integer. \textsc{Learning with expert advice} is an
online problem in which an algorithm chooses a point $p_t$ in $N$-dimensional
probability simplex $\Delta_N = \{ x \in \R^N ~:~ x \ge 0, \|x\|_1 = 1 \}$ and
receives a loss\footnote{The reason for using loss, instead of reward, will be
aparent in Section~\ref{section:reduction-experts}.} vector $\ell_t \in
[0,1]^N$. Similarly as before, the goal of the algorithm is to minimize its
regret after $T$ rounds with respect to any competitor $u \in \Delta_N$, which
is defined as
$$
\Regret_T(u) = \sum_{t=1}^T \langle \ell_t, p_t \rangle - \sum_{t=1}^T \langle \ell_t, u \rangle \; .
$$

\subsection{Binary and Continuous Coin Betting}

We consider a gambler making repeated bets on outcomes of adversarial coin
flips. The gambler starts with an initial endowment $\epsilon > 0$. In each
round $t$, he bets on an outcome of a coin flip $g_t \in \{-1,1\}$ where $+1$
denotes heads and $-1$ denotes tails. We do not make any assumption on how $g_t$
is generated, that is, it can be chosen by an adversary.

The gambler can bet any amount on either heads or tails. However, he is not
allowed to borrow any money. If he loses (i.e. he bets on the incorrect
outcome), he loses the betted amount. If he wins the bet (i.e. guesses the
outcome correctly), he gets twice the betted amount back.  Note that it does not
make sense for the gambler to bet on both outcomes, since the same winnings can
be achieved by betting the difference of the two amounts on one of the outcomes
and zero on the other outcome. We encode gambler's bet in round $t$ by a single
number $\beta_t \in (-1,1)$. The sign of $\beta_t$ encodes whether he is betting
on heads or tails. The absolute value encodes the betted amount as the fraction
of his current wealth.

Let $\Wealth_t$ be gambler's wealth at the end of round $t$. It satisfies the
following recurrence satisfies
\begin{align}
\label{equation:wealth-recurrence}
\Wealth_0 & = \epsilon &
& \text{and} &
\Wealth_t & = (1 + g_t \beta_t) \Wealth_{t-1} \qquad \text{for $t \ge 1$} \; .
\end{align}
Note that since $\beta_t \in (-1,1)$, gambler's wealth is always positive.
Gambler's net reward (difference of wealth and initial endowment) after $t$
rounds is
\begin{align}
\label{equation:reward-wealth}
\Reward_t = \Wealth_t - \ \epsilon \; .
\end{align}

We generalize the problem slighlty by allowing the outcome of the coin flip
$g_t$ to be any real number in the interval $[-1,1]$. In this case we talk about
\emph{betting on a continuous coin}. The formulas for $\Wealth_t$ and
$\Reward_t$ remain the same.
