\section{Online Learning Algorithms Based on the Krichevsky-Trofimov Estimator}
\label{section:kt-estimator}

In this section we show that the optimal Krichevsky-Trofimov (KT) estimator has associated a potential, which we call \emph{KT potential}.
We then prove regret bounds for \ac{OLO} over Hilbert space and \ac{LEA}, obtaining optimal regret bounds.


\citet{KrichevskyT81} proposed that after seeing coin flips $g_1, g_2, \dots, g_{t-1}$ you bet a fraction of your money equal to
\begin{equation}
\label{equation:kt-estimator-betting-strategy}
\beta_t =  \tfrac{\sum_{i=1}^{t-1} g_i}{t} \; .
\end{equation}
\citeauthor{KrichevskyT81} proved the following guarantee on the wealth obtained by such strategy
$$
\forall \beta \in [-1,1] \qquad \qquad \ln(\Wealth_t) \ge \ln(\Wealth_t(\beta)) \ - \ \tfrac{1}{2} \ln t \ - \ \ln(2) \; ,
$$
where $\Wealth_t(\beta)$ is the wealth of a strategy that bets the same fraction
$\beta$ in every round. This guarantee is optimal up to constant additive factors~\citep{Cesa-BianchiL06}.

We will use a generalized version of the KT estimator.
First, define the \emph{shifted KT potential} with parameter $\delta$ as
$$
F_t(x) = \tfrac{2^t \cdot \Gamma\left(\delta + 1 \right) \cdot \Gamma\left(\tfrac{t+\delta+1}{2} + \tfrac{x}{2} \right) \cdot \Gamma\left(\tfrac{t+\delta+1}{2} - \tfrac{x}{2} \right)}{\Gamma\left(\tfrac{\delta+1}{2} \right)^2 \cdot \Gamma \left(t+\delta+1\right)} \; .
$$
where $\Gamma(x) = \int_0^\infty t^{-x} e^{-t} dt$ is Euler's gamma function.
The reason for its name is apparent from the corresponding betting fraction (from point 3 in Definition~\ref{}) that is
$\beta_t = \tfrac{\sum_{j=1}^{t-1} g_j}{\delta+t}$.
Hence, the shifted KT potential is shifted in time by $\delta$, i.e., $t$ is replaced by $\delta+t$.
When $\delta=1$ we recover the original KT scheme.
It is possible to prove that the shifted KT potentials form a sequence of coin-betting potentials for initial endowment $1$.



% \begin{algorithm}[t]
% \caption{Algorithm for OLO over Hilbert space $\H$ based on KT potential
% \label{algorithm:kt-hilbert-space-olo}}
% \begin{algorithmic}
% {
% \REQUIRE{Initial endowment $\epsilon > 0$}
% %\STATE{Initialize $\Wealth_0 \leftarrow \epsilon$}
% \FOR{$t=1,2,\dots$}
% \STATE{Predict with $w_t \leftarrow \tfrac{1}{t} \left(\epsilon + \sum_{i=1}^{t-1} \langle g_i, w_i \rangle \right) \sum_{i=1}^{t-1} g_i$}
% %\STATE{Predict $w_t$}
% \STATE{Receive reward vector $g_t \in \H$ such that $\norm{g_t} \le 1$}
% %\STATE{Update $\Wealth_t \leftarrow \Wealth_{t-1} \ + \ \langle g_t, w_t \rangle$}
% \ENDFOR
% }
% \end{algorithmic}
% \end{algorithm}

\noindent\textbf{OLO in Hilbert Space.}
We apply the KT shifted potential with $\delta=1$ for the construction of an OLO algorithm over a Hilbert
space $\H$. According to \eqref{equation:hilbert-space-olo} and simplifying the expression of $\beta_t$, the resulting algorithm predicts in round $t$,
$$
w_t
= \tfrac{1}{t} \Wealth_{t-1} \sum_{i=1}^{t-1} g_i
= \tfrac{1}{t} \left(\epsilon + \sum_{i=1}^{t-1} \langle g_i, w_i \rangle \right) \sum_{i=1}^{t-1} g_i \; .
$$
%The algorithm is stated as Algorithm~\ref{algorithm:kt-hilbert-space-olo}.

We derive a regret bound for Algorithm~\ref{algorithm:kt-hilbert-space-olo} by
applying Theorem~\ref{theorem:hilbert-space-olo-regret-bound} to the KT
potential \eqref{equation:kt-estimator-potential}. The regret bound is stated as
Corollary~\ref{corollary:kt-hilbert-space-olo-regret} below.
%Its proof can be
%found in the Appendix~\ref{section:corollaries_reductions}. The only technical
%part of the proof is an upper bound on Fenchel conjugate $F_t^*$ since it cannot
%be expressed as an elementary function.

\begin{corollary}[Regret Bound for Algorithm~\ref{algorithm:kt-hilbert-space-olo}]
\label{corollary:kt-hilbert-space-olo-regret}
Let $\epsilon > 0$. Let $\{g_t\}_{t=1}^\infty$ be any sequence of reward vectors
in a Hilbert space $\H$ such that $\norm{g_t} \le 1$.
Algorithm~\ref{algorithm:kt-hilbert-space-olo} satisfies
$$
\forall \, T \ge 0 \quad
\forall u \in \H \qquad \qquad
\Regret_T(u) \le \norm{u} \sqrt{T \ln\left(1 + \tfrac{4T^2 \norm{u}^2}{\epsilon^2} \right)} + \epsilon \left(1 - \tfrac{1}{2\sqrt{T}} \right) \;.
$$
\end{corollary}

\noindent\textbf{Learning with Expert Advice.}
We will construct an algorithm for Learning with Expert Advice based on
shifted KT potential, with $\delta=T/2$, where $T$ is the number of rounds $T$ that has to be known in advance. Later, we will remove this asspumption.

Recall that for construction of the final algorithm, we need, as an intermediate
step, an OLO algorithm for one-dimensional Hilbert space $\R$. This algorithm
predicts for any sequence $\{g_t\}_{t=1}^\infty$ of reward vectors,
$$
w_t
= \beta_t \Wealth_{t-1}
= \beta_t \left(1 + \sum_{j=1}^{t-1} g_j w_j \right)
= \tfrac{\sum_{i=1}^{t-1} g_i}{T/2+t} \left(1 + \sum_{j=1}^{t-1} g_j w_j \right) \; .
$$
%Following the construction in Section~\ref{section:reduction-experts}, we arrive
%at the final algorithm, Algorithm~\ref{algorithm:kt-experts}.

We can derive a regret bound for Algorithm~\ref{algorithm:kt-experts} by
applying Theorem~\ref{theorem:regret-bound-experts} to the shifted KT potential.
The result is stated as Corollary~\ref{corollary:kt-experts-regret} below.
%The
%proof of the corollary is in the Appendix~\ref{section:corollaries_reductions}.
%The technical part of the proof is an upper bound on $f_t^{-1}(x)$, which we
%conveniently do by lower bounding $F_t(x)$.
The reason for using the shifted
potential comes from the analysis of $f_t^{-1}(x)$. The unshifted algorithm would
have $O(\sqrt{T (\log T + \KL{u}{\pi}})$ regret bound; shifting improves the
bound to $O(\sqrt{T (1 + \KL{u}{\pi}})$.

\begin{corollary}[Regret Bound for Algorithm~\ref{algorithm:kt-experts}]
\label{corollary:kt-experts-regret}
Let $N \ge 2$ and $T \ge 0$ be integers. Let $\pi \in \Delta_N$ be a prior.
For any sequence $\ell_1, \ell_2, \dots, \ell_T \in
[0,1]^N$ of loss vectors, Algorithm~\ref{algorithm:kt-experts}
with input $N,T,\pi$ satisfies\footnote{By changing $T/2$ to another constant
fraction of $T$, it is possible to trade-off between the two constants $3$ and
$4$ present in the square root.}
$$
\forall u \in \Delta_N \qquad \qquad \Regret_T(u) \le \sqrt{3T (4 + \KL{u}{\pi})} \; .
$$
\end{corollary}


The requirement of knowing the number of rounds $T$ in advance can be lifted by
the standard doubling trick~\citep[Section 2.3.1]{Shalev-Shwartz12}. We obtain
an anytime algorithm at the expense of slightly worse regret bound,
$$
\forall \, T \ge 0 \quad \forall u \in \Delta_N \qquad \qquad
\Regret_T(u) \le \tfrac{\sqrt{2}}{\sqrt{2} - 1} \sqrt{3T (4 + \KL{u}{\pi})} \; .
$$

%Also, as observed by \citet{ChernovV10}, bounds in terms of the KL
%divergence are superior to the $\epsilon$-quantile bounds.
