\section{Introduction}
\label{section:introduction}

We consider the \ac{OLO}~\citep{Cesa-Bianchi-Lugosi-2006, Shalev-Shwartz-2011}
over a Hilbert space $\H$. In each round $t$, an algorithm chooses a point $w_t
\in \H$ and then receives a loss vector $\ell_t \in \H$. The algorithm's goal is
to keep its \emph{regret} small, defined as the difference between its
cumulative reward and the cumulative reward of a fixed strategy $u \in \H$,
that is
\[
\Regret_T(u) = \sum_{t=1}^T \langle \ell_t, w_t \rangle - \sum_{t=1}^T \langle \ell_t, u \rangle  \; .
\]
We focus on one particular decision set, the Hilbert space $\H$.  We assume
that $\norm{\ell_t} \le 1$.

\ac{OLO} is a basic building block of many machine learning problems. For
example, \ac{OCO} is a problem analogous to \ac{OLO} where
the linear function $u \mapsto \langle \ell_t, u \rangle$
is generalized to an arbitrary convex function $f_t(u)$. \ac{OCO} is solved
through a reduction to \ac{OLO} by feeding the algorithm $\ell_t = \grad
f_t(w_t)$~\citep{Shalev-Shwartz-2011}.  Batch and stochastic convex
optimization can also be solved through a reduction to \ac{OLO} by
taking the average of $w_1, w_2, \dots, w_T$~\citep{Shalev-Shwartz-2011}.

To achieve optimal regret, most of the existing online algorithms (e.g.  Online
Gradient Descent) require the user to set the learning rate to an
unknown/oracle value. Recently, new parameter-free algorithms have been
proposed for \ac{OLO}/\ac{OCO} over Hilbert
spaces~\citep{Streeter-McMahan-2012, Orabona-2013, McMahan-Abernethy-2013,
McMahan-Orabona-2014, Orabona-2014, Orabona-Pal-2016-parameter-free}.  These algorithms adapt to the norm of the
optimal predictor, without the need to tune parameters. However, their
\emph{design and underlying intuition} is still a challenge.

Our contributions are as follows. We connect algorithms for \ac{OLO} with coin
betting. Namely, we show an algorithm for \ac{OLO} can be viewed as an
algorithm for betting on outcomes of adversarial coin flips. The wealth the
algorithm can generate for the betting problem is connected to the regret in
\ac{OLO} setting. This insight allows us to design novel parameter-free
algorithms, which are extremely simple and natural. We also show some
applications of our results to convex optimization and machine learning.

\noindent\textbf{Notation and Definitions.}
We will use the following notation in the rest of this note.  $\H$ denotes
a real Hilbert space, $\langle \cdot, \cdot \rangle$ is its inner product and
$\norm{\cdot} = \sqrt{\langle \cdot, \cdot \rangle}$ is its induced norm.
