\section{Introduction}
\label{section:introduction}

We consider the \ac{OLO}~\citep{Cesa-Bianchi-Lugosi-2006, Shalev-Shwartz-2011}
over a Hilbert space $\H$. In each round $t$, an algorithm chooses a point $w_t
\in \H$ and then receives a reward vector $g_t \in \H$. The algorithm's goal is
to keep its \emph{regret} small, defined as the difference between its
cumulative reward and the cumulative reward of a fixed strategy $u \in \H$,
that is
\[
\Regret_T(u) = \sum_{t=1}^T \langle g_t, u \rangle - \sum_{t=1}^T \langle g_t, w_t \rangle \; .
\]
We focus on one particular decision set, the Hilbert space $\H$.  We assume
that $\norm{g_t} \le 1$.

\ac{OLO} is a basic building block of many machine learning problems. For
example, \ac{OCO}, the problem analogous to \ac{OLO} where $\langle \ell_t, u
\rangle$ is generalized to an arbitrary convex function $f_t(u)$, is solved
through a reduction to \ac{OLO}~\citep{Shalev-Shwartz-2011}.
Batch and stochastic convex optimization
can also be solved through a reduction to \ac{OLO}~\citep{Shalev-Shwartz-2011}.

To achieve optimal regret, most of the existing online algorithms (e.g.  Online
Gradient Descent) require the user to set the learning rate to an
unknown/oracle value. Recently, new parameter-free algorithms have been
proposed for \ac{OLO}/\ac{OCO} over Hilbert
spaces~\citep{Streeter-McMahan-2012, Orabona-2013, McMahan-Abernethy-2013,
McMahan-Orabona-2014, Orabona-2014}.  These algorithms adapt to the norm of the
optimal predictor, without the need to tune parameters. However, their
\emph{design and underlying intuition} is still a challenge.

Our contributions are as follows. We connect algorithms for \ac{OLO} with coin
betting. Namely, we show an algorithm for \ac{OLO} can be viewed as an
algorithm for betting on outcomes of adversarial coin flips. The wealth the
algorithm can generate for the betting problem is connected to the regret in
\ac{OLO} setting. This insight allows us to design novel parameter-free
algorithms, which are extremely simple and natural. We also show some
applications of our results to convex optimization and machine learning.

\noindent\textbf{Notation and Definitions.}
We will use the following notation in the rest of this note.  If $\H$ is a real
Hilbert space, $\langle \cdot, \cdot \rangle$ is its inner product and
$\norm{\cdot}$ its induced norm.
