\documentclass[wcp]{jmlr}

\usepackage[nolist]{acronym}
\usepackage{algorithm,algorithmic}
\usepackage{times}
\usepackage{enumerate}

\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Wealth}{Wealth}
\DeclareMathOperator{\Reward}{Reward}
\DeclareMathOperator{\polylog}{polylog}

\newcommand{\N}{\mathbb{N}}     % natural numbers
\newcommand{\R}{\mathbb{R}}     % real numbers
\newcommand{\C}{\mathbb{C}}     % complex numbers
\renewcommand{\H}{\mathcal{H}}  % Hilbert space
\newcommand{\KL}[2]{D\left({#1}\middle\|{#2}\right)}  % KL divergence
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\indicator}{\mathbf{1}}

\begin{acronym}
\acro{EG}{Exponentiated Gradient}
\acro{DFEG}{Dimension-Free Exponentiated Gradient}
\acro{OMD}{Online Mirror Descent}
\acro{ASGD}{Averaged Stochastic Gradient Descent}
\acro{GD}{Gradient Descent}
\acro{SGD}{Stochastic Gradient Descent}
\acro{PiSTOL}{Parameter-free STOchastic Learning}
\acro{OCO}{Online Convex Optimization}
\acro{OLO}{Online Linear Optimization}
\acro{RKHS}{Reproducing Kernel Hilbert Space}
\acro{IID}{Independent and Identically Distributed}
\acro{SVM}{Support Vector Machine}
\acro{ERM}{Empirical Risk Minimization}
\acro{COCOB}{Continous Coin Betting}
\acro{MBA}{Master Betting Algorithm}
\acro{KT}{Krichevsky-Trofimov}
\acro{LEA}{Learning with Expert Advice}
\acro{FTRL}{Follow The Regularized Leader}
\end{acronym}


\jmlrvolume{1}
\jmlryear{2016}
\jmlrworkshop{ICML 2016 AutoML Workshop}

\author{%
\Name{Francesco Orabona}
\Email{francesco@orabona.com}
\AND
\Name{D\'avid P\'al}
\Email{dpal@yahoo-inc.com}\\
\addr Yahoo Research, New York}


\title{From Coin Betting to Parameter-Free Online Learning}

\begin{document}

\maketitle

\begin{abstract}
We present two new algorithms for online linear optimization.  The first is an
algorithm for unbounded online linear optimization over a Hilbert space.  The
second is an algorithm for learning with expert advice. Both algorithms are
simple and are easy to implement.  The regret of the first algorithm is as good
as regret of gradient descent with best possible learning rate. The regret of
the second algorithm is as good as the regret of the well-known Hedge algorithm
with best possible learning rate. A unified analysis of both algorithms can be
given via adversarial coin-betting game, Kelly betting and Krichevsky-Trofimov
estimator.
\end{abstract}

\input{introduction}
\input{learning-rates}
\input{hilbert-space-olo}
\input{experts}
\input{design-and-analysis}
%\input{coin-betting-potentials}
%\input{reduction-hilbert-space}
%\input{reduction-experts}
%\input{kt-estimator}
\input{discussion}

% Acknowledgments---Will not appear in anonymized version
\acks{The authors thank Jacob Abernethy, Nicol\`{o} Cesa-Bianchi, Satyen Kale, Chansoo Lee, and Giuseppe Molteni for useful discussions on this work.}

\bibliography{learning}

\appendix
%\input{appendix-coin-betting}
%\input{appendix-reductions}
%\input{appendix-kt-potential}
%\input{appendix-kt-reductions}


\end{document}
