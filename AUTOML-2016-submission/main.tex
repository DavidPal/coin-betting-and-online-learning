\documentclass[wcp]{jmlr}

\usepackage[nolist]{acronym}
\usepackage{algorithm,algorithmic}
\usepackage{times}
\usepackage{enumerate}

\DeclareMathOperator*{\Exp}{\mathbf{E}}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Wealth}{Wealth}
\DeclareMathOperator{\Reward}{Reward}
\DeclareMathOperator{\polylog}{polylog}

\newcommand{\N}{\mathbb{N}}     % natural numbers
\newcommand{\R}{\mathbb{R}}     % real numbers
\newcommand{\C}{\mathbb{C}}     % complex numbers
\renewcommand{\H}{\mathcal{H}}  % Hilbert space
\newcommand{\KL}[2]{D\left({#1}\middle\|{#2}\right)}  % KL divergence
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\indicator}{\mathbf{1}}
\newcommand{\scO}{\mathcal{O}}  % big-Oh

\begin{acronym}
\acro{EG}{Exponentiated Gradient}
\acro{DFEG}{Dimension-Free Exponentiated Gradient}
\acro{OMD}{Online Mirror Descent}
\acro{ASGD}{Averaged Stochastic Gradient Descent}
\acro{OGD}{Online Gradient Descent}
\acro{SGD}{Stochastic Gradient Descent}
\acro{PiSTOL}{Parameter-free STOchastic Learning}
\acro{OCO}{Online Convex Optimization}
\acro{OLO}{Online Linear Optimization}
\acro{RKHS}{Reproducing Kernel Hilbert Space}
\acro{IID}{Independent and Identically Distributed}
\acro{SVM}{Support Vector Machine}
\acro{ERM}{Empirical Risk Minimization}
\acro{COCOB}{Continous Coin Betting}
\acro{MBA}{Master Betting Algorithm}
\acro{KT}{Krichevsky-Trofimov}
\acro{LEA}{Learning with Expert Advice}
\acro{FTRL}{Follow The Regularized Leader}
\end{acronym}


\jmlrvolume{1}
\jmlryear{2016}
\jmlrworkshop{ICML 2016 AutoML Workshop}

\author{%
\Name{Francesco Orabona}
\Email{francesco@orabona.com}
\AND
\Name{D\'avid P\'al}
\Email{dpal@yahoo-inc.com}\\
\addr Yahoo Research, New York}

\title{Parameter-Free Convex Learning through Coin Betting}

\begin{document}

\maketitle

\begin{abstract}
We present a new parameter-free algorithm for online linear optimization over a
Hilbert space. It is theoretically optimal, with regret guarantees as good as
with the best possible learning rate. The algorithm is simple and easy to
implement. The analysis is given via the adversarial coin-betting game, Kelly
betting and the Krichevsky-Trofimov estimator. Applications to obtain
parameter-free convex optimization and machine learning algorithms are shown.
\end{abstract}

\input{introduction}
\input{learning-rates}
\input{algorithm}
\input{applications}
\input{experiments}

% Acknowledgments---Will not appear in anonymized version
\acks{The authors thank Jacob Abernethy, Nicol\`{o} Cesa-Bianchi, Satyen Kale, Chansoo Lee, and Giuseppe Molteni for useful discussions on this work.}

\bibliography{learning}

\end{document}
