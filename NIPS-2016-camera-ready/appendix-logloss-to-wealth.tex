\section{From Log Loss to Wealth}
\label{section:logloss-to-wealth}

Guarantees for betting or sequential investement algorithm are often expressed
as upper bounds on the regret with respect to the log loss.  Here, for the sake
of completeness, we show how to convert such a guarantee to a lower bound on
the wealth of the corresponding betting algorithm.

We consider the problem of predicting a binary outcome.  The algorithm predicts
at each round probability $p_t \in [0,1]$. The adversary generates a sequences
of outcomes $x_t \in \{0,1\}$ and the algorithm's loss is
\[
\ell(p_t,x_t) = -x_t \ln p_t -(1-x_t) \ln (1-p_t) \; .
\]
We define the regret with respect to a fixed probability vector $\beta$ as
\[
\Regret^{\mathrm{logloss}}_T = \sum_{t=1}^T \ell(p_t,x_t) - \min_{\beta \in [0,1]} \sum_{t=1}^T \ell(\beta,x_t) \; .
\]

\begin{lemma}
Assume that an algorithm that predicts $p_t$ guarantees
$\Regret^{\mathrm{logloss}}_T \leq R_T$.  Then, the coin betting strategy with
endowement $\epsilon$ and $\beta_t = 2 p_{t}-1$ guarantees
\[
\Wealth_T \ge \epsilon \exp\left(T \cdot \KL{\frac{1}{2} + \frac{\sum_{t=1}^T g_t}{2 T}}{\frac{1}{2}} - R_T \right)
\]
against any sequence of outcomes $g_t \in [-1,+1]$.
\end{lemma}

\begin{proof}
Define $x_t=\tfrac{1+g_t}{2}$. We have
\begin{align*}
\ln \Wealth_T
& = \ln (\Wealth_{t-1} + w_t g_t) \\
& = \ln (\Wealth_{t-1}(1 + g_t \beta_t))\\
& = \ln \epsilon \prod_{t=1}^T (1 + g_t\beta_t) \\
& = \ln \epsilon + \sum_{t=1}^T \ln (1 + g_t\beta_t)\\
& \ge \ln \epsilon +  \sum_{t=1}^T \left( \frac{1+g_t}{2} \right) \ln \left(1 + \beta_t\right) + \left( \frac{1-g_t}{2} \right) \ln \left(1 - \beta_t \right) \\
& =  \ln \epsilon + \sum_{t=1}^T \left( \frac{1+g_t}{2} \right) \ln \left(2p_t \right) + \left( \frac{1-g_t}{2} \right) \ln \left(2 (1 - p_t) \right) \\
& =  \ln \epsilon + T \ln(2) + \sum_{t=1}^T \left( \frac{1+g_t}{2} \right) \ln (p_t) + \left( \frac{1-g_t}{2} \right) \ln (1 - p_t) \\
& =  \ln \epsilon + T \ln(2) - \sum_{t=1}^T \ell(p_t, x_t) \\
& =  \ln \epsilon + T \ln(2) - \Regret^{\mathrm{logloss}}_T - \min_{\beta \in [0,1]} \sum_{t=1}^T \ell(\beta,x_t) \\
& \ge  \ln \epsilon + T \ln(2) - R_T - \min_{\beta \in [0,1]} \sum_{t=1}^T \ell(\beta,x_t) \; ,
\end{align*}
where the first inequality is due to the concavity of $\ln$ and the second one
is due to the assumption of the regret.

It is easy to see that the $\beta^*=\argmin_{\beta \in [0,1]} \sum_{t=1}^T
\ell(\beta,x_t)=\tfrac{\sum_{t=1}^T x_t}{T}$. Hence, we have
\[
\min_{\beta \in [0,1]} \sum_{t=1}^T \ell(\beta,x_t) = T \left( - \beta^* \ln \beta^* - (1-\beta^*) \ln (1-\beta^*)\right) \; .
\]
Also, we have that for any $\beta \in [0,1]$
\[
- \beta \ln \beta - (1-\beta) \ln (1-\beta) = - \KL{\beta}{\frac{1}{2}} + \ln 2 \; .
\]

Putting all together, we have the stated lemma.
\end{proof}

The lower bound on the wealth of the adaptive Kelly betting based on the KT
estimator is obtained simply by the stated Lemma and reminding that the log
loss regret of the KT estimator is upper bounded by $\frac{1}{2}\ln T + \ln 2$.
